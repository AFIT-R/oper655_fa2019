---
title: "DocSummary_Example"
author: "2nd Lt. Speranza, 2nd Lt. Gold, 1st Lt. Buck"
date: "November 6, 2018"
output: html_document
---

#In Class Example


Now we will walk through a few examples of document summarization (aka automatic summarization).

##LexRankr Example

LexRankr can be used to recreate a text summarizer used on Reddit posts.

##Get Nessasary Packages
These are the nessasary packages needed for the in class example.  You can use Pacman in order to install and load in the packages.
```{r}
pacman::p_load("xml2",
               "rvest",
               "lexRankr")
```


##Load Data Sets
The data sets we will be using are  
```{r}
#url to scrape
monsanto_url = "https://www.theguardian.com/environment/2017/sep/28/monsanto-banned-from-european-parliament"

#read page html
page = xml2::read_html(monsanto_url)
#extract text from page html using selector
page_text = rvest::html_text(rvest::html_nodes(page, ".js-article__body p"))
```
Now that the data has been read in, we can perform the lexRank method to obtain the top sentenses. This is an extractive technique and can be useful in getting summaries directly from the article. 

```{r}
#perform lexrank for top 3 sentences
top_3 = lexRankr::lexRank(page_text,
                          #only 1 article; repeat same docid for all of input vector
                          docId = rep(1, length(page_text)),
                          #return 3 sentences to mimick /u/autotldr's output
                          n = 3,
                          continuous = TRUE)

#reorder the top 3 sentences to be in order of appearance in article
order_of_appearance = order(as.integer(gsub("_","",top_3$sentenceId)))
#extract sentences in order of appearance
ordered_top_3 = top_3[order_of_appearance, "sentence"]
ordered_top_3
```

This is a very simple way to obtain extractive summaries from various articles and documents.  


##TextRank
###Jane Austen

We will start by packages that have been developed to help with TextRank method.  This is the same algorithm that google uses to pick out summaries of web documents. [(3)](https://www.hvitfeldt.me/2018/03/tidy-text-summarization-using-textrank/)

First, we will install and prepare all of the nessasary packages.

```{r}
pacman::p_load("tidyverse",
               "tidytext",
               "textrank",
               "rvest")
```

Now we can pull in the data from the url and pull in with tibble. Our tokenizer is sentences. Then we can tokenize again to get the indivdiual words. Finally, we remove stop words

```{r}
url <- "http://time.com/5196761/fitbit-ace-kids-fitness-tracker/"
article <- read_html(url) %>%
  html_nodes('div[class="padded"]') %>%
  html_text()

#get sentences
article_sentences <- tibble(text = article) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)
head(article_sentences)

#get words
article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

#remove stop words
article_words <- article_words %>%
  anti_join(stop_words, by = "word")
```

Now that we have the proper format for textrank, there are only two inputs required for the software to run. 
*A data.frame with sentences
*A data.frame with words
Here is the code and the output for summarizing the article.

```{r}
article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)
article_summary
```

Lastly, we can look at where these sentences took place in the article. 
```{r}
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank)) +
  geom_col() +
  theme_minimal()
```

It is interesting to note that the 4 most important sentences in the first half of the article. This logically makes sense because the company writing the article will want to give the reader the information up front.  

This example online goes on to see if the same technique would work for a book rather than an article.  

Lets get the janeausten book and run the analysis. 

```{r}
pacman::p_load("janeaustenr")

#load in the book and split by chapter
emma_chapters <- janeaustenr::emma %>%
  tibble(text = .) %>%
  mutate(chapter_id = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  filter(chapter_id > 0) %>%
  group_by(chapter_id) %>%
  summarise(text = paste(text, collapse = ' '))

#remove stop words. 
emma_words <- emma_chapters %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")
```

Now we can use the same code as above to analyze using the text rank software in R and print the graph to see which chapters have the most important information according to the Text Rank algorithm.  

```{r}
emma_summary <- textrank_sentences(data = emma_chapters, 
                                   terminology = emma_words)
#creates the plot
emma_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Chapter",
       y = "TextRank score",
       title = "Chapter importance in the novel Emma by Jane Austen") +
  scale_x_continuous(breaks = seq(from = 0, to = 55, by = 5))
```

  There really isn't that much information you can gather from this. No chapter or chapters seems to have a clear importance over the other ones.  

##Using both Methods on Star Wars Movie Script
  Both of the examples above were found online. Now we know how each of the packages work, we can try and use these on our own examples.  

  First we need to download the data set. Obtained through kaggle, we will be using the movie script for Star Wars Episode 4.  The data cleaning done on this is very similar to how we cleaned the data for the Harry Potter Books.

```{r}
#read in the movie scripts'
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(tm, 
               pdftools, 
               here,
               tau,
               tidyverse,
               stringr,
               tidytext, 
               RColorBrewer,
               qdap,
               qdapRegex,
               qdapDictionaries,
               qdapTools,
               data.table,
               coreNLP,
               scales,
               text2vec,
               SnowballC,
               DT,
               quanteda,
               RWeka,
               broom,
               tokenizers,
               grid,
               knitr,
               widyr)
library(readr)

Episode4<-read_file("File/SW_EpisodeIV.txt")
Episode4<-gsub('\\\"'," ",Episode4)

clean <- tibble::tibble(text = Episode4) %>%
    tidytext::unnest_tokens(sentence, text,token ='sentences') %>%
    dplyr::mutate(movie = "Episode4") %>%
    dplyr::select(movie, dplyr::everything())
  
  Ep4_tidy <-clean
  


head(Ep4_tidy)

#gets rid of the line number
Ep4_tidy$sentence<-gsub(Ep4_tidy$sentence,
      pattern = "[0-9]",
      replacement = ""
      )
head(Ep4_tidy$sentence)
```
###Lex Rank Method. 
  Now that our data is somewhat clean, we can use the LexRank method to see what the most important sentences are. Instead of trying to summarize the whole movie with just three lines, we will summarize the movie into 3 parts. The beginning, middle, and end. 

```{r}
Ep4Summary=NULL
for (i in 1:4){
  

top_3Ep4 = lexRankr::lexRank(Ep4_tidy$sentence[(i-1)*round(nrow(Ep4_tidy)/4)+1:round(nrow(Ep4_tidy)/4)*i],
                          #only 1 article; repeat same docid for all of input vector
                          docId = rep(1, round(length(Ep4_tidy$sentence[1:nrow(Ep4_tidy)])/4)),
                          #return 3 sentences
                          n = 3,
                          continuous = TRUE)

#reorder the top 3 sentences to be in order of appearance in article
order_of_appearance = order(as.integer(gsub("_","",top_3Ep4$sentenceId)))
#extract sentences in order of appearance
ordered_top_3 = top_3Ep4[order_of_appearance, "sentence"]
Ep4Summary<-cbind(Ep4Summary,ordered_top_3)
}

Ep4Summary
```
  Again, this is an extractive summarization technique so full sentences are pulled from the data set exactly how it appears in the text.  While this might not summarize the movie very well for someone who has never seen star wars, this does a good job of picking out key lines from the movie that would jog someone's memory of what happened in the movie.  

  Now lets see what results we get when we use the TextRank method on the same dataset. 

###TextRank on Star Wars Episode 4

  First we will need to get data in a smiliar format as above. We already have the sentences, now we just need to tokenize them by word as well. 
```{r}
library(textrank)
#get sentences
Ep4_sentences <- tibble(text = Episode4) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)


Ep4_sentences$sentence<-gsub('[[:digit:]]+',' ',Ep4_sentences$sentence)
head(Ep4_sentences)

Part1_sen<-Ep4_sentences[1:round(nrow(Ep4_sentences)/4),]
#get words
Part1_words <- Part1_sen %>%
  unnest_tokens(word, sentence)

#remove stop words
Part1_words <- Part1_words %>%
  anti_join(stop_words, by = "word")
head(Part1_words)

```
  
  Now that we have the document split up into the first quarter in the same way as the Jane Austen example, we can use the textrank_sentences function to pull out what textrank thinks the most important sentences are for the first quarter of the movie.  
  

```{r}
Part1_summary <- textrank_sentences(data = Part1_sen, 
                                  terminology = Part1_words)
Part1_summary

Part1_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank)) +
  geom_col() +
  theme_minimal() +
  labs(x = "Line",
      y = "TextRank score",
       title = "Script Line from the first quarter of Star Wars Episode 4") +
  scale_x_continuous(breaks = seq(from = 0, to = 55, by = 5))

```
  As you can see, the textrank method and Lexrank method produced vastly different results for finding the most important lines in the Star Wars Episode 4 script.  While TextRank did an alright job in summarizing the the short article about fitbit, the algorithm struggled in summarizing the larger documents with the Jane Austen and Star Wars script.  

  Our hope is that now you know enough about these methods and how to format them that everyone will feel comfortable in using these summarization techniques in their own final project.  

#References
1. [LexRank Example for Online Article Summarization](https://adamspannbauer.github.io/2017/12/17/summarizing-web-articles-with-r/)
2. [TextRank By Thomas W. Jones](https://cran.r-project.org/web/packages/textmineR/vignettes/e_doc_summarization.html)
3. [TextRank example for Books](https://www.hvitfeldt.me/2018/03/tidy-text-summarization-using-textrank/)
