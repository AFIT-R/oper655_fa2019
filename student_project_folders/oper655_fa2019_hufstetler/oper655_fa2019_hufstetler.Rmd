---
title: "OPER 655 Student Project Report"
author: "Capt Brandon Hufstetler"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  html_document:
    code_folding: 'hide'
abstract: 'This is where you put your abstract'
---

```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = F,
                      warning = F, 
                      comment = NA)
```

## Project Overview/Problem Statement 

Describe your project in sufficient detail for someone who is not familiar with text mining or NLP can understand what you are saying. This section should conclude with a problem statement that clearly and concisely describes the problem you are addressing (aka the question you are trying to answer). 

### Installing Loading Required R Packages

Before going too far, be sure your reader has the necessary packages to follow along with your methodology and ultimately reproduce your results.

```{r, warning=FALSE, message=TRUE}
#install.packages("pacman")
pacman::p_load(tm,
               tidytext,
               XML,
               rvest,
               RCurl,
               rprojroot,
               qdapTools,
               pdftools,
               antiword,
               glue,
               data.table,
               tidyverse,
               vroom,
               antiword,
               magick,
               tesseract,
               readr,
               gsubfn)
```

## Methodology 

OUTLINE GOES HERE

PIPELINE -> Ingest Data
          -> Regex Parsing 
            -> VS Representation 
              -> StopWords (common and specific)

While it wasn't used for this project, a function, ingest(), was created to examine all documents in the given folder of type csv, txt, doc, pdf, jpg, or tif and will create variables in the global R environment containing the raw text from each of them.
```{r, warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}
ingest <- function(dest){
root <- rprojroot::find_root(rprojroot::is_rstudio_project)

#Update file path to the files folder
#dest <- file.path(root, "student_project_folders", "oper655_fa2019_hufstetler", "Files")

#Create List of All Files in Folder
master_list <- list.files(path = dest,
                          pattern = "",
                          full.names = TRUE)

#Create List of PDF Files
pdf_files <- list.files(path = dest, 
                        pattern = "pdf",
                        full.names = TRUE)

#Convert PDFs to txt
if (length(pdf_files)>0){
lapply(pdf_files,
       FUN = function(x) system(glue::glue("pdftotext {x}"), wait = FALSE))
}

#Create list of csv files
csv_files <- list.files(path = dest,
              pattern = "csv",
              full.names = TRUE)

#Create List of Word Files
ms_files <- list.files(path = dest, 
                       pattern = "docx?",
                       full.names = TRUE)

#Create List of txt files
text_files <- list.files(path = dest,
                         pattern = "txt",
                         full.names = TRUE)

#List of Image Files
image_files <- list.files(path = dest,
                          pattern = "jpg|tif",
                          full.names = TRUE)

#Read in .csv Files (https://stackoverflow.com/questions/11433432/how-to-import-multiple-csv-files-at-once)
if (length(csv_files)>0){
for (i in 1:length(csv_files)){
  assign(paste("cs",sprintf("%02d", i),sep = "_"), vroom::vroom(csv_files[i]), envir = .GlobalEnv)
}}

#Read in .doc* files
if (length(ms_files)>0){
for (i in 1:length(ms_files)){
  assign(paste("ms", sprintf("%02d", i), sep = "_"), qdapTools::read_docx(ms_files[i]), envir = .GlobalEnv)
}}

#Read in .txt Files
if (length(text_files)>0){
for (i in 1:length(text_files)){
  assign(paste("tx", sprintf("%02d", i), sep = "_"), read_file(text_files[i]), envir = .GlobalEnv)
}}

#Import Image Files
if (length(image_files)>0){
for (i in 1:length(image_files)){
 assign(paste("im",sprintf("%02d", i),sep = "_"), image_read(image_files[i]) %>%
           image_resize("2000") %>%
           image_convert(colorspace = 'gray') %>%
           image_trim() %>%
           image_ocr(), envir = .GlobalEnv)
}}
}

```

### Data 

Describe the data

The data for this project consists of the scripts for all 187 episodes in the TV series, The Office. The scripts are found on individual webpages indexed at https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=the-office-us. Unfortunately, the scripts were not easily accessible on their own. An inspection of the site elements revealed that the scripts were presented in a page divider with the id, "scrolling-script-container". The contents of the containers included the script and just a few html formatting items. Knowing this, it was simple to extract the script and remove the unwanted items. The process of compiling all of the data for this project is outlined below.

The XML, RCurl, rprojroot, and gsubfn packages are used to extract and clean the data for this project. A directory location was determined within the project directory and assigned to the variable "save_folder". The data directory be named "Data" and created in my student folder in the course repository.
```{r}
pacman::p_load(XML,
               RCurl,
               rprojroot,
               gsubfn,
               rvest,
               stringr)
```
```{r, eval=FALSE}
proj_root   <- find_root(is_rstudio_project)
save_folder <- file.path(proj_root,'student_project_folders','oper655_fa2019_hufstetler','Data')
dir.create(save_folder)
```

The script data was found online but was spread across 186 websites. Luckily these sites were all indexed on a single page. The URLs for that page and its root were assigned to the variables "url" and "url_root".

```{r}
url_root <- 'https://www.springfieldspringfield.co.uk/'
url  <- 'https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=the-office-us'
```

The html and xml content are extracted from the index site and parsed. The attributes separated by anchor tags are parsed and just the href attributes are isolated into the variable "hrefs".

```{r}
# Extract html/xml content from URL
rcurl.txt <- RCurl::getURL(url,
                           .opts = RCurl::curlOptions(followlocation = TRUE))
  substr(rcurl.txt,1,50)
# Parse html content
url_parsed <- XML::htmlParse(rcurl.txt, asText = TRUE)

# We need to get the href attributes from
# the anchor tags <a> stored on the page
attrs <- XML::xpathApply(url_parsed, "//a", XML::xmlAttrs)
  attrs[1:2]
# Next, we'll split out the hrefs
# from the other attributes
hrefs <- sapply(seq_along(attrs), FUN = function(x) attrs[[x]][['href']])
  hrefs[39:42]
```

Looking at all of the hrefs on this site, common only among the ones of interest is the feature "view". All links containing the feature "view" are put into the variable "episodes" and then reattached to the root url in the variable "files". 

```{r}
# Then, we only want the hrefs for the files
# that have a .docx file extension
episodes  <- hrefs[grep('view',hrefs)]
  episodes[1:2]
# Construct a list of URL's for each file
# by pasting two character strings together
files <- paste0(url_root, episodes)
  files[1]
```

For each url in files, the site text is extracted using the read_html(), html_nodes(), and html_text() functions. Each site bounds the text of interest within a div attribute with the id "scrolling-script-container". A few html format features are then removed using stringr tools and the text extraction and cleaning process is complete. The file() command is then used to create a .txt file in the data directory for each of the cleaned episode texts. Before executing the file creation process, the first url in files is tested.
```{r}
scraping_scripts <- read_html(files[1])

text <- scraping_scripts %>%
        html_nodes("div.scrolling-script-container") %>%
        html_text() %>%
        str_replace_all(pattern = "\\s-\\s|\n|\r|\t|\"", replacement = " ") %>%
        str_trim(side = "both")
substr(text,1,200)
```
It worked so the process is applied to all urls in files. Each file is named based on the season and episode that its contents came from.
```{r, eval=FALSE}
for(i in files) {
  scraping_scripts <- read_html(i)

  text <- scraping_scripts %>%
          html_nodes("div.scrolling-script-container") %>%
          html_text() %>%
          str_replace_all(pattern = "\\s-\\s|\n|\r|\t|\"", replacement = " ") %>%
          str_trim(side = "both")
  
  fileConn<-file(paste(save_folder,"/", substr(i, nchar(i)-5, nchar(i)),".txt", sep=""))
  writeLines(text, fileConn)
  close(fileConn)
}


```

### Analysis

Walk your reader through you analysis.  The yaml header is already set to fold all code chunks. 

## Findings and Conclusions 

Describe your findings and conclusions here.  Include your important visualizations

### Future Work

What else would you do if you had more time?

