---
title: "Document Summarization"
author: "Trey Pujats, Maria Schroeder, Clarence Williams"
date: "November 8, 2019"
header-includes:
  - \usepackage{graphicx,fancyhdr,amsmath,amssymb,amsthm,subfig, mathtools}
output: 
  html_document:
    toc: yes
    toc_float: yes
    css: 'css/hufstetler.css'
bibliography:  document_summarization.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, 
                      comment = NA, 
                      message = FALSE,
                      warning = FALSE,
                      eval = T)
```

## Setup

This first set of code lays down the foundation for our analysis. The data is imported and cleaned (credit: Huf) for document summarization application.

## Overview
Document summarization's origins can traced back to the 1950's @Luhn . The paper "The automatic creation of literature abstracts", introduced a method to extract sentences from the text using features such as wordand phrase frequency. The goal of the initial research was to summarize scientific documents @Allahyari. 

Document summarization's ultimate goal is to identify the most important and mostfrequent text within a document and ouput those results, capturing the best summary of the document. Document summarization can be categorized as either abstractive or extractive.

Abstractive document summarization uses semantic relationships between text in the original document and uses natural language processing to generate new text to describe the summarization of the document. Abstractive summarization is generally the way that humans create summaries. Abstractive summarizers can be built using ANN.

The link below walks through building an abstractive document summarizizes in python using Keras.
<a href="https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/"> Comprehensive Guide to Text Summarization using Deep Learning in Pythont</a>



Typically document summarization techniques do not use this kind of summarization because problems of semantic representation and others are difficult and currently not easy to deal with. In contrast, extractive ranks the text within the document against one another and then extracts the most important text/ highest ranking textin the document. It chooses a subset of sentences to output verbatim from the original text as the summary. In general the steps of Document Summarization can be summarized as follows:

1. Construct immediate representation of the document (Topic Representation, Indicator (Feature) Representaion)
     
2. Score the sentences based on the representation

3. Select a summary of the text using k sentences

This document discuses three methods to perfrom document summarization: weighted word frenquecy, LSA and  text rank. 



Practical Applications:

```{r, echo= FALSE}

customer_summarizer <- function(article){
  print("This is trash")
 
}

```


# Practical Example
```{r}

pacman::p_load(XML,
               rvest,
               RCurl,
               rprojroot,
               qdapTools,
               pdftools,
               antiword,
               glue,
               data.table,
               tidyverse,
               vroom,
               antiword,
               magick,
               tesseract,
               tools,
               readxl,
               here,
               tm,
               text2vec,
               LSAfun,
               igraph,
               textmineR,
               tidytext)


EpisodeScripts<-vector()

Season8<-rep("Season_8",6)


url  <- 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=game-of-thrones&episode=s08e06'
rcurl.doc <- RCurl::getURL(url,
                           .opts = RCurl::curlOptions(followlocation = TRUE))
url_parsed <- XML::htmlParse(rcurl.doc, asText = TRUE)


EpisodeScripts[1]<-xpathSApply(url_parsed, "//div[@class='scrolling-script-container']", xmlValue)

EpisodeScripts <- stringr::str_replace_all(EpisodeScripts, "<br */>", "")
EpisodeScripts <- stringr::str_replace_all(EpisodeScripts, "\r", "")
EpisodeScripts <- stringr::str_replace_all(EpisodeScripts, "\t", "")
EpisodeScripts <- stringr::str_replace_all(EpisodeScripts, "\n", "")
EpisodeScripts <- stringr::str_replace_all(EpisodeScripts, "\"", "")


customer_summarizer(EpisodeScripts)

```


```{r}
genericSummary(EpisodeScripts,3)
```




# Cleaning the Data

Since Captain Brandon Hufstetler cleaned the code for text sentiment, we did not duplicate efforts although we alterred thim to fit the document summarization setup. Instead, we did not unnest each word but kept each review together. This made it easier to categorize the data and evaluate complete sentences rather than words for sentiment. The finished dataset is review_tidy and it is shown below. Next we decided to look at each phone separate from one another. Summaries of different phones from different companies does not capture anything useful to the consumer or the producer in this case. Subsets were created to split the data by phone including the iPhone4, iPhone5, iPhone6, iPhone7, galaxy S5, galaxy s6, galaxy s7, and galaxy s8. Using these subsets, we performed document summarization.
```{r, Cleaning the data}
pacman::p_load(tidyr,
               tidytext,
               tidyverse,
               textdata,
               plyr,
               dplyr,
               stringr,
               ggplot2,
               magrittr,
               wordcloud,
               reshape2,
               textmineR,
               LSAfun,
               igraph,
               textrank,
               ggraph,
               lattice,
               udpipe)



root <- rprojroot::find_root(rprojroot::is_rstudio_project)
file_loc <- file.path(root,"data","phone_user_reviews")

file_list <- list.files(path = file_loc,
                        pattern = "",
                        full.names = TRUE)
reviews_tidy <- tibble::tibble()
manu_pattern <- "/cellphones/[a-z0-9]+"
prod_pattern <- paste(manu_pattern, "-|/", sep = "")
for (i in file_list){
  input <- load(i,ex <- new.env())
  text_raw <- get(ls(ex),ex)
  text_en <- text_raw[text_raw$lang=="en",]
  rm(ex, text_raw, input, i)
  
  clean <- tibble::tibble(score = text_en$score,
                          maxscore = text_en$score_max,
                          text = text_en$extract,
                          product = gsub(prod_pattern, "", text_en$phone_url),
                          author = text_en$author,
                          manufacturer = gsub("/cellphones/","",str_extract(text_en$phone_url,manu_pattern))) 
  
  reviews_tidy <- base::rbind(reviews_tidy, clean)
  rm(text_en, clean)
}
rm(file_list, root, manu_pattern, prod_pattern, file_loc)
reviews_tidy <- select(reviews_tidy, -maxscore)



GalaxyS5<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s5"))
GalaxyS6<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s6"))
GalaxyS7<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s7-edge"))
GalaxyS8<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s8"))

iPhone4<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-4"))
iPhone5<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-5"))
iPhone6<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-6"))
iPhone7<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-7"))

reviews_tidy_note <- reviews_tidy[grep("note", reviews_tidy$product), ]
#unique(reviews_tidy_note$product)

reviews_tidy_note7 <- reviews_tidy_note[grep("note-7", reviews_tidy_note$product), ]




iPhone4_WorstReviews<-na.omit(subset(iPhone4, iPhone4$score<=3))
iPhone4_BestReviews<-na.omit(subset(iPhone4, iPhone4$score>=8))

iPhone5_WorstReviews<-na.omit(subset(iPhone5, iPhone5$score<=3))
iPhone5_BestReviews<-na.omit(subset(iPhone5, iPhone5$score>=8))

iPhone6_WorstReviews<-na.omit(subset(iPhone6, iPhone6$score<=3))
iPhone6_BestReviews<-na.omit(subset(iPhone6, iPhone6$score>=8))

iPhone7_WorstReviews<-na.omit(subset(iPhone7, iPhone7$score<=3))
iPhone7_BestReviews<-na.omit(subset(iPhone7, iPhone7$score>=8))


  
iPhone4_WorstReviews<-ddply(iPhone4_WorstReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone4_BestReviews<-ddply(iPhone4_BestReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone5_WorstReviews<-ddply(iPhone5_WorstReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone5_BestReviews<-ddply(iPhone5_BestReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone6_WorstReviews<-ddply(iPhone6_WorstReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone6_BestReviews<-ddply(iPhone6_BestReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone7_WorstReviews<-ddply(iPhone7_WorstReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
iPhone7_BestReviews<-ddply(iPhone7_BestReviews, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
  
reviews_tidy_note7_unnest <-ddply(reviews_tidy_note7, .(product, manufacturer), summarize,
           Review_full=paste(text,collapse=" "))
  
  
iPhone4_WorstReviewstb<-na.omit(subset(iPhone4, iPhone4$score<=3))
iPhone4_BestReviewstb<-na.omit(subset(iPhone4, iPhone4$score>=8))

iPhone5_WorstReviewstb<-na.omit(subset(iPhone5, iPhone5$score<=3))
iPhone5_BestReviewstb<-na.omit(subset(iPhone5, iPhone5$score>=8))

iPhone6_WorstReviewstb<-na.omit(subset(iPhone6, iPhone6$score<=3))
iPhone6_BestReviewstb<-na.omit(subset(iPhone6, iPhone6$score>=8))

iPhone7_WorstReviewstb<-na.omit(subset(iPhone7, iPhone7$score<=3))
iPhone7_BestReviewstb<-na.omit(subset(iPhone7, iPhone7$score>=8))
  

  
    
```


# Weighted Word Frequency
The code below tokenizes the sentences then get the frequency of each word and divides each words frequency by the frequency of the most occuring word. @python_Sum

```{python}

import bs4 as bs
import urllib.request
import re
import nltk
import pandas as pd


#print(r.reviews_10_collaspe(1,3))

def weight_wordfreq_summarizer(doc, numSentences):
  article_text = re.sub(r'\[[0-9]*\]', ' ', doc.loc[0, 'Review_full'])
  article_text = re.sub(r'\s+', ' ', doc.loc[0, 'Review_full'])


  # Removing special characters and digits
  formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)
  formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)


  #tokenize sentences
  sentence_list = nltk.sent_tokenize(article_text)

  #Find Weighted Frequency of Occurrence
  stopwords = nltk.corpus.stopwords.words('english')

  word_frequencies = {}
  for word in nltk.word_tokenize(formatted_article_text):
      if word not in stopwords:
          if word not in word_frequencies.keys():
              word_frequencies[word] = 1
          else:
              word_frequencies[word] += 1

  #get weighted frequency
  maximum_frequncy = max(word_frequencies.values())

  for word in word_frequencies.keys():
      word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)

  #Calculate sentence scores
  sentence_scores = {}
  for sent in sentence_list:
      for word in nltk.word_tokenize(sent.lower()):
          if word in word_frequencies.keys():
              if len(sent.split(' ')) < 30:
                  if sent not in sentence_scores.keys():
                      sentence_scores[sent] = word_frequencies[word]
                  else:
                      sentence_scores[sent] += word_frequencies[word]
                    
  #Get the summary
  import heapq
  summary_sentences = heapq.nlargest(numSentences, sentence_scores, key=sentence_scores.get)

  summary = ' '.join(summary_sentences)
  print(summary)
  return





```

```{python}
#summarizer(r.galaxys5_WorstReviewss_collaspe, 5)
weight_wordfreq_summarizer(r.reviews_tidy_note7_unnest, 5)

```


# Text Rank Summary
The most common is the text ranking method. The text ranking method breaks the document into text strings of the users choice and measures similarity between these text stuructures (consider them sentences for the purpose of this presentation). Graphically, these sentences create vertices and the frequency of similarity of sentences is measured by edges that connect to vertices within the graph. Furthermore, the edges can have weight to them based on how important the vertex is that is connected to the edge. A vertex of high importance is meaured by its similarity to other vertices and the similarity is measured by lexical or semantic relations to the other text. Rada Mihalcea shows the four steps to perform text ranking in document summarization @mihalcea:

1. Identify text units that best define the task at hand,and add them as vertices in the graph.

2. Identify relations that connect such text units, and use these relations to draw edges between vertices in the graph. Edges can be directed or undirected, weighted or unweighted.

3. Iterate the graph-based ranking algorithm until convergence.

4. Sort vertices based on their final score. Use the values attached to each vertex for ranking/selection decisions.There are many different ways to score sentences. Methods of scoring use location of words, words in titles, indicator phrases, and/or cue methods. The use of location assumes greater importance to sentences at the beginning and end of sentences. The use of titles heuristics assume a higher importance of sentences that contain keywords from titles. There are certain words that imply or accompany words that help summarize a given report. An example of this may be the phrase "To conclude". Additionally, cue words can either imply importance of a sentence or its lack of importance. An example of a word that implies greater imporance is "significant". A word that may imply lesser importance is "hardly". Based off of these methods, sentences can be scored from these heuristics. In our dataset, sincethere are no subsections and it is not a single large document some of these may not be helpful.




The function textrank_sentences of the textRank package implements the text rank algorithm.



This function uses jaccard distances as the similiarity measure which is defined as follows:

$ 1 - \frac{|A \cap B|}{|A \cup B|}$




```{r, Galaxy S5}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.

textrank_summary <- function(article, numSentences){
  
  article_sentences <- tibble(text = article$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank))%>% 
  slice(1:numSentences) %>%
  pull(sentence)
  
}

```

Lets look at five sentences that summarize the Note 7 reviews by using  the text rank.

```{r}

detach("package:plyr", unload=TRUE)

textrank_summary(reviews_tidy_note7,5)

```



# Latent Semantic Analysis

LSA applies the method of @Liu for generic text summarization of text document D:

From our Topic Modeling Lecture:
One way to think about this process is that we are keeping the *t* most important dimensions, where *t* is a number we choose ahead of time based on how many topics we want to extract.

$$
A = U_tS_tV_t^T
$$

The $U$ matrix is in the term space and the V matrix is in the document space. The columns correspond to each of our topics. So if *t* is two, we keep two columns of each. With these matrices, we can then apply cosine similarity or other measures.


The method of Document Summariziation using LSA is as follows:

  1. Decompose the document D into individual sentences, and use these sentences to form the candidate sentence set S, and set k = 1.

  2. Construct the terms by sentences matrix A for the document D.
  
  3. Perform the SVD on A to obtain the singular value matrix $\Sigma$ and the right singular vector matrix $V^T$. In the singular  vector space, each sentence i is represented by the column vector $\psi_i = [v_{i1}, v_{i2}, \dots v_{ir}]$ of $V^T$
  
  4. Select the $k^{th}$ right singular vector from matrix   $V^T$
  
  5. Select the sentence which has the largest index value with the k'th right singular vector, and include it in the summary.

  6. If k reaches the predefined number, terminate the operation; otherwise, increment k by one, and go to Step 4

Step 5 is equivalent to finding the the column vector $\psi_i$ whose $k^{th}$ element $v_{ik}$ is the largest. This process is finding the best senetence to describe the topic that respsented by the $k^{th}$ singular vector of $\psi_i$. @Liu
 
 

The function genericSummary() of the LSAfun package implements the LSA to summarize text.


Let's summarize the Note 7 Reviews using this technique

```{r}

genericSummary(reviews_tidy_note7_unnest[1,3],5)
```





# iPhone Reviews

## iPhone 4 Reviews
```{python}
weight_wordfreq_summarizer(r.iPhone4_WorstReviews, 5)

```



```{python}
weight_wordfreq_summarizer(r.iPhone4_BestReviews, 5)

```

## iPhone 5 Reviews
```{python}
#summarizer(r.galaxys5_WorstReviewss_collaspe, 5)
#weight_wordfreq_summarizer(r.iPhone5_BestReviews, 5)
#textrank_summary(iPhone5_BestReviews,5)

```


```{python}
#summarizer(r.galaxys5_WorstReviewss_collaspe, 5)
#genericSummary(iPhone5_BestReview$Review_full,5)
#weight_wordfreq_summarizer(r.iPhone5_WorstReviews, 5)

```



## iPhone 6 Reviews

```{python}

weight_wordfreq_summarizer(r.iPhone6_WorstReviews, 5)

```


```{python}
weight_wordfreq_summarizer(r.iPhone6_BestReviews, 5)

```



## iPhone 7 Reviews
```{python}
weight_wordfreq_summarizer(r.iPhone7_WorstReviews, 5)

```

```{python}

weight_wordfreq_summarizer(r.iPhone7_BestReviews, 5)

```


# Battery Life

Since battery is such a hot topic for Galaxy, the reviews for the Samsung Galaxy phones were reduced to reviews that mention battery life using the "str_detect()" function. In the interest of time and processing power, these reviews were reduced to 100 (for each phone generation). The first five sentences that summarize the reviews about battery life where reported. 

The same process was used as before with TextRank

## S5 Battery Life 
```{r}
article_sentences <- tibble(text = GalaxyS5$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 200 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)

#This shows us the top three sentences that summarize the document
GalaxyS5_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)


print("Galaxy S5")
GalaxyS5_battery_summary

```


## S6 Battery Life 
```{r, echo=FALSE}
#S6
article_sentences <- tibble(text = GalaxyS6$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 100 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
GalaxyS6_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS6$text[1:100],3)


print("Galaxy S6")
GalaxyS6_battery_summary
```

## S7 Battery Life 
```{r, echo=FALSE}
#S7
article_sentences <- tibble(text = GalaxyS7$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 200 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
GalaxyS7_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
# genericSummary(GalaxyS7$text[1:100],3)


print("Galaxy S7")
GalaxyS7_battery_summary


```

## S8 Battery Life 

```{r, echo=FALSE}
#S8
article_sentences <- tibble(text = GalaxyS8$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 100 obs

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
GalaxyS8_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS8$text[1:100],3)

print("Galaxy S8")
GalaxyS8_battery_summary

```

## iPhone Battery life

```{r, echo=FALSE}
article_sentences <- tibble(text = iPhone4$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 100 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
iPhone4_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS8$text[1:100],3)

print("iPhone4")
iPhone4_battery_summary
```

```{r, echo=FALSE}
article_sentences <- tibble(text = iPhone6$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 100 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
iPhone6_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS8$text[1:100],3)

print("iPhone6")
iPhone6_battery_summary
```

```{r, echo=FALSE}
article_sentences <- tibble(text = iPhone7$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 100 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
iPhone7_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS8$text[1:100],3)

print("iPhone7")
iPhone7_battery_summary
```

# Note 7 Battery Life

```{r, echo=FALSE}
article_sentences <- tibble(text = reviews_tidy_note7$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"battery"),]
#take only 100 obs


article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
reviews_tidy_note7_battery_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS8$text[1:100],3)

print("Note 7")
reviews_tidy_note7_battery_summary
```

#Lets check a camera
```{r}
article_sentences <- tibble(text = GalaxyS5$text) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

##ADDED:Takes only reveiews that mention battery
article_sentences=article_sentences[str_detect(article_sentences$sentence,"camera"),]
#take only 100 obs
article_sentences=article_sentences[sample(nrow(article_sentences), 100),]

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
GalaxyS5_camera_summary=article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:5) %>%
  pull(sentence)
##
#genericSummary(GalaxyS8$text[1:100],3)

print("GalaxyS5 camera")
GalaxyS5_camera_summary
```



# References