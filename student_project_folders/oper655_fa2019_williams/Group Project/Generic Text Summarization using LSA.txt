Generic Text Summarization Using Relevance Measure and
                                                Latent Semantic Analysis
                                   Yihong Gong                                                               Xin Liu
                NEC USA, C & C Research Laboratories                                      NEC USA, C & C Research Laboratories
                              110 Rio Robles Drive                                                    110 Rio Robles Drive
                               San Jose, CA 95134                                                     San Jose, CA 95134
                         ygong@ccrl.sj.nec.com                                                      xliu@ccrl.sj.nec.com
ABSTRACT                                                                              help the user to sift through vast volumes of information,
In this paper, we propose two generic text summarization                              and to quickly identify the most relevant documents.
methods that create text summaries by ranking and extract-                               With a large volume of text documents, presenting the
ing sentences from the original documents. The rst method                             user with a summary of each document greatly facilitates
uses standard IR methods to rank sentence relevances, while                           the task of nding the desired documents. Text search and
the second method uses the latent semantic analysis tech-                             summarization are the two essential technologies that com-
nique to identify semantically important sentences, for sum-                          plement each other. Text search engines return a set of
mary creations. Both methods strive to select sentences that                          documents that seem to be relevant to the user's query, and
are highly ranked and di erent from each other. This is                               text summarizers produce document summaries that enable
an attempt to create a summary with a wider coverage of                               quick examinations through the returned documents. Text
the document's main content and less redundancy. Perfor-                              search engines serve as information lters that sift out an ini-
mance evaluations on the two summarization methods are                                tial set of relevant documents, while text summarizers serve
conducted by comparing their summarization outputs with                               as information spotters that help users to spot the nal set
the manual summaries generated by three independent hu-                               of desired documents.
man evaluators. The evaluations also study the in uence                                  Text summaries can be either query-relevant summaries
of di erent VSM weighting schemes on the text summariza-                              or generic summaries. A query-relevant summary presents
tion performances. Finally, the causes of the large dispar-                           the contents of the document that are closely related to
ities in the evaluators' manual summarization results are                             the initial search query. Creating a query-relevant sum-
investigated, and discussions on human text summarization                             mary is essentially a process of retrieving the query relevant
patterns are presented.                                                               sentences/passages from the document, which has a strong
                                                                                      analogy with the text retrieval process. Therefore, query-
                                                                                      relevant summarization is often achieved by extending con-
Keywords                                                                              ventional IR technologies, and to date, a large number of
                                                                                      text summarizers in the literature fall into this category.
Generic Text Summarization, Relevance Measure, Latent                                 On the other hand, a generic summary provides an overall
Semantic Analysis                                                                     sense of the document's contents. A good generic summary
                                                                                      should contain the main topics of the document while keep-
1.     INTRODUCTION                                                                   ing redundancy to a minimum. As no query nor topic will be
   The explosive growth of the world-wide web has dramat-                             provided to the summarization process, it is challenging to
ically increased the speed and the scale of information dis-                          develop a high quality generic summarization method, and
semination. With a vast sea of accessible text documents on                           is even more challenging to objectively evaluate the method.
the Internet, conventional IR technologies have become more                              In this paper, we propose two generic text summarization
and more insucient for nding relevant information e ec-                              methods that create text summaries by ranking and extract-
tively. Nowadays, it is quite common that a keyword-based                             ing sentences from the original documents. The rst method
search on the Internet returns hundreds, or even thousands                            uses standard IR methods to measure sentence relevances,
of hits, by which the user is often overwhelmed. Therefore,                           while the second method uses the latent semantic analysis
there is an increasing need for new technologies that can                             technique to identify semantically important sentences, for
                                                                                      summary creations. Both methods strive to select sentences
                                                                                      that are highly ranked and di erent from each other. This
                                                                                      is an attempt to create a summary with a wider coverage of
Permission to make digital or hard copies of all or part of this work for             the document's main content and less redundancy. Perfor-
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies            mance evaluations on the two summarization methods are
bear this notice and the full citation on the first page. To copy otherwise, to       conducted by comparing their summarization outputs with
republish, to post on servers or to redistribute to lists, requires prior specific    the manual summaries generated by three independent hu-
permission and/or a fee.                                                              man evaluators. The evaluations also study the in uence
SIGIR’01, September 9-12,2001,New Orleans, Louisiana, USA.
Copyright 2001 ACM 1-58113-331-6/01/0009 ... 5.00.   $                                of di erent VSM weighting schemes on the text summariza-
                                                                                   19

tion performances. Finally, the causes of the large dispar-       3. CREATING GENERIC SUMMARIES
ities in the evaluators' manual summarization results are            Query-relevant text summaries are useful for answering
investigated, and discussions on human text summarization         such questions as whether a given document is relevant to
patterns are presented.                                           the user's query, and if relevant, which part(s) of the doc-
   The remainder of the paper consists of four sections: Sec-     ument is relevant. As query-relevant summaries are query
tion 2 describes related research studies, Section 3 describes    biased, they do not provide an overall sense of the docu-
the two proposed text summarization methods, Section 4            ment content, and hence, are not appropriate for content
presents the performance evaluations, and Section 5 sum-          overview. For answering such questions as which category
marizes the paper.                                                the document belongs to, and what are the key points of
                                                                  the document, generic text summaries must be created and
2.    RELATED WORK                                                presented to the reader.
   Text summarization has been actively researched in re-            A document usually consists of several topics. Some top-
cent years. A majority of the research studies in the lit-        ics are described intensively by many sentences, and hence
erature have been focused on creating query-relevant text         form the major content of the document. Other topics may
summaries. M. Sanderson proposed a query-relevant sum-            just be brie y mentioned to supplement the major topics,
marizer that divides the document into equally sized over-        or to make the whole story more complete. A good generic
lapping passages, and uses the INQUERY text search engine         summary should cover the major topics of the document as
to obtain the passage that best matches the user's query.         much as possible, and at the same time, keep redundancy
This best passage is then used as a summary of the docu-          to a minimum.
ment [1]. A query expansion technique called Local Context           In this section, we propose two methods that create
Analysis (LCA, which is also from INQUERY) is used before         generic summaries by selecting sentences based on the rele-
the best passage retrieval. Given a topic and a document col-     vance measure and the latent semantic analysis. Both meth-
lection, the LCA retrieves top-ranked documents from the          ods need to rst decompose the document into individual
collection, examines the context surrounding the topic terms      sentences, and to create a weighted term-frequency vector
in each retrieved document, and then selects and adds the         for each of the sentences. Let Ti = [t1i t2i : : : tni ]T be
words/phrases that are frequent in this context to the query.     the term-frequency vector of passage i, where element tji
B. Baldwin and T.S. Morton developed a summarizer that            denotes the frequency in which term j occurs in passage i.
selects sentences from the document until all the phrases         Here passage i could be a phrase, a sentence, a paragraph of
in the query are covered. A sentence in the document is           the document, or could be the whole document itself. The
considered to cover a phrase in the query if they co-refer to     weighted term-frequency vector Ai = [a1i a2i : : : ani ]T of
the same individual, organization, event, etc [2]. R. Barzi-      passage i is de ned as:
lay and M. Elhadad developed a method that creates text                              aji = L(tji ) G(tji )                  (1)
summaries by nding lexical chains from the document [3].
                                                                                                  
The Cornell/Sabir system uses the document ranking and            where L(tji ) is the local weighting for term j in passage i,
passage retrieval capabilities of the SMART text search en-       and G(tji ) is the global weighting for term j in the whole
gine to e ectively identify relevant passages in a document       document. When the weighted term-frequency vector Ai
[4]. The text summarizer from CGI/CMU uses a technique            is created, we further have the choice of using Ai with its
called Maximal Marginal Relevance (MMR) which measures            original form, or normalizing it by its length Ai . There
                                                                                                                   j  j
the relevance of each sentence in the document to the user        are many possible weighting schemes. In Section 4.3, we
provided query, as well as to the sentences that have been se-    inspect several major weighting schemes and disclose how
lected and added into the summary [5]. The text summary           these weighting schemes a ect the summarization perfor-
is created by selecting the sentences that are highly rele-       mances.
vant to the user's query, but are di erent from each other.          In the following subsections, the two text summarization
The SUMMARIST text summarizer from the University of              methods are described in details.
Southern California strives to create text summaries based
on the equation: summarization =topic identi cation + in-         3.1 Summarization by Relevance Measure
terpretation +generation. The identi cation stage lters the          After the given document is decomposed into individual
input document to determine the most important, central           sentences, we compute the relevance score of each sentence
topics. The interpretation stage clusters words and ab-           with the whole document. We then select the sentence k
stracts them into some encompassing concepts. Finally, the        that has the highest relevance score, and add it to the sum-
generation stage generates summaries either by outputting         mary. Once the sentence k has been added to the summary,
some portions of the input, or by creating new sentences          it is eliminated from the candidate sentence set, and all the
based on the interpretation of the document concepts [6].         terms contained in k are eliminated from the original doc-
However, this generation function was not realized in the pa-     ument. For the remaining sentences, we repeat the steps
per. The Knowledge Management (KM) system from SRA                of relevance measure, sentence selection, and term elimina-
International, Inc. extracts summarization features using         tion until the number of selected sentences has reached the
morphological analysis, name tagging and co-reference res-        prede ned value. The operation ow is as follows:
olution. They used a machine learning technique to deter-
mine the optimal combination of these features in combina-            1. Decompose the document into individual sentences,
tion with statistical information from the corpus to identify            and use these sentences to form the candidate sentence
the best sentences to include in a summary [7].                          set S .
                                                               20

    2. Create the weighted term-frequency vector Ai for each       which represents the weighted term-frequency vector of sen-
       sentence i S , and the weighted term-frequency vec-
                       2                                           tence i, to column vector i = [vi1 vi2         vir ]T of matrix
       tor D for the whole document.                                   T
                                                                   V , and maps each row vector j in matrix A, which tells
                                                                   the occurrence count of the term j in each of the documents,
    3. For each sentence i S , Compute the relevance score
                                 2                                 to row vector 'j = [uj1 uj2      ujr ] of matrix U. Here each
       between Ai and D, which is the inner product between        element vix of i , ujy of 'j is called the index with the x th,
                                                                                                                                 0
       Ai and D.                                                   y th singular vectors, respectively.
                                                                     0
                                                                       From semantic point of view, the SVD derives the latent
    4. Select sentence k that has the highest relevance score,     semantic structure from the document represented by ma-
       and add it to the summary.                                  trix A [9]. This operation re ects a breakdown of the orig-
    5. Delete k from S , and eliminate all the terms contained     inal document into r linearly-independent base vectors or
       in k from the document. Recompute the weighted              concepts. Each term and sentence from the document is
       term-frequency vector D for the document.                   jointly indexed by these base vectors/concepts. A unique
                                                                   SVD feature which is lacking in conventional IR technolo-
    6. If the number of sentences in the summary reaches the       gies is that the SVD is capable of capturing and modeling
       prede ned value, terminate the operation; otherwise,        interrelationships among terms so that it can semantically
       go to Step 3.                                               cluster terms and sentences. Consider the words doctor,
                                                                   physician, hospital, medicine, and nurse. The words doctor
In Step 4 of the above operations, sentence k that has the         and physician are synonyms, and hospital, medicine, nurse
highest relevance score with the document is the one that          are the closely related concepts. The two synonyms doc-
best represents the major content of the document. Select-         tor and physician generally appear in similar contexts that
ing sentences based on their relevance scores ensures that         share many related words such as hospital, medicine, nurse,
the summary covers the major topics of the document. On            etc. Because of these similar patterns of word combinations,
the other hand, eliminating all the terms contained in k           the words doctor and physician will be mapped near to each
from the document in Step 5 ensures that the subsequent            other in the r-dimensional singular vector space. Further-
sentence selection will pick the sentences with a minimum          more, as demonstrated in [10], if a word combination pattern
overlap with k. This leads to the creation of a summary            is salient and recurring in a document, this pattern will be
that contains little redundancy.                                   captured and represented by one of the singular vectors. The
                                                                   magnitude of the corresponding singular value indicates the
3.2 Summarization By Latent Semantic Anal-                         importance degree of this pattern within the document. Any
        ysis                                                       sentences containing this word combination pattern will be
   Inspired by the latent semantic indexing, we applied the        projected along this singular vector, and the sentence that
singular value decomposition (SVD) to generic text summa-          best represents this pattern will have the largest index value
rization. The process starts with the creation of a terms by       with this vector. As each particular word combination pat-
sentences matrix A = [A1 A2 : : : An ] with each column            tern describes a certain topic/concept in the document, the
vector Ai representing the weighted term-frequency vector          facts described above naturally lead to the hypothesis that
of sentence i in the document under consideration. If there        each singular vector represents a salient topic/concept of
are a total of m terms and n sentences in the document, then       the document, and the magnitude of its corresponding sin-
we will have an m n matrix A for the document. Since
                                                                  gular value represents the degree of importance of the salient
every word does not normally appear in each sentence, the          topic/concept.
matrix A is usually sparse.                                            Based on the above discussion, we propose the following
   Given an m n matrix A, where without loss of generality
                                                                  SVD-based document summarization method.
m n, the SVD of A is de ned as [8]:
    
                                                                        1. Decompose the document D into individual sentences,
                               A = UVT                     (2)            and use these sentences to form the candidate sentence
                                                                           set S , and set k = 1.
where U = [uij ] is an m n column-orthonormal ma-
                                    
trix whose columns are called left singular vectors;  =                2. Construct the terms by sentences matrix A for the
diag(1 ; 2 ; : : : ; n ) is an n n diagonal matrix whose di-
                                                                          document D.
agonal elements are non-negative singular values sorted in              3. Perform the SVD on A to obtain the singular value
descending order, and V = [vij ] is an n n orthonormal
                                               
                                                                           matrix , and the right singular vector matrix VT . In
matrix whose columns are called right singular vectors. If                 the singular vector space, each sentence i is represented
rank(A)=r, then  satis es                                                 by the column vector i = [vi1 vi2      vir ]T of VT .
            1 2         r > r+1 = = n = 0:
                                                         (3)         4. Select the k'th right singular vector from matrix VT .
   The interpretation of applying the SVD to the terms by               5. Select the sentence which has the largest index value
sentences matrix A can be made from two di erent view-                     with the k'th right singular vector, and include it in
points. From transformation point of view, the SVD derives                 the summary.
a mapping between the m-dimensional space spanned by the
weighted term-frequency vectors and the r-dimensional sin-              6. If k reaches the prede ned number, terminate the op-
gular vector space with all of its axes linearly-independent.              eration; otherwise, increment k by one, and go to Step
This mapping projects each column vector i in matrix A,                    4.
                                                                21

                                     Table 1: Particulars of the Evaluation Database
                                         Document Attributes                    Values
                                                      number of docs                 549
                                                 number of docs with                 243
                                             more than 10 sentences
                                                   avg sentences/doc                  21
                                                   min sentences/doc                   3
                                                   max sentences/doc                 105
                               Table 2: Statistics of the Manual Summarization Results
                         Summarization Attributes                Values                Average Sentences/Doc
                             Total number of sentences             7053                         29.0
                         Sentences selected by 1 person            1283                          5.3
                        Sentences selected by 2 persons             604                          2.5
                        Sentences selected by 3 persons             290                          1.2
                     Total number of selected sentences            2177                          9.0
In Step 5 of the above operation, nding the sentence that            sentences selected by at least one of the evaluators. Table 2
has the largest index value with the k'th right singular vector      shows the statistics of the manual summarization results.
is equivalent to nding the column vector i whose k'th ele-           As evidenced by the table, the disagreements among the
ment vik is the largest. By the hypothesis, this operation is        three evaluators were much more than expected: each docu-
equivalent to nding the best sentence describing the salient         ment has an average of 9.0 sentences selected by at least one
concept/topic represented by the k'th singular vector. Since         evaluator, and among these 9.0 selected sentences, only 1.2
the singular vectors are sorted in descending order of their         sentences receive a unanimous vote from all three evaluators.
corresponding singular values, the k'th singular vector rep-         Even when the sentence selection is determined by a major-
resents the k'th important concept/topic. Because all the            ity vote, we still get a lower than expected overlapping rate:
singular vectors are independent of each other, the sentences        an average of 2.5 sentences per document. The disparities
selected by this method contain the minimum redundancy.              became even larger with longer documents. These statistics
                                                                     suggest that for many documents in the database, their man-
4.    PERFORMANCE EVALUATION                                         ual summarization determined by a majority vote could be
   In this section, we describe the data corpus constructed          very short (2.5 sentence per document), and this summary
for performance evaluations, present various evaluation re-          length is below the best xed summary length (three to ve
sults, and make in-depth observations on some aspects of             sentences) suggested in [5]. For this reason, we decided to
the evaluation outcomes.                                             evaluate our two summarization methods using each of the
                                                                     three individual manual summarization results, as well as
4.1 Data Corpus                                                      the combined result determined by a majority vote.
   Our evaluations on the two proposed summarization                 4.2 Performance Evaluations
methods have been conducted using a database of two
months of the CNN Worldview news programs. Exclud-                      We used the recall (R), precision (P), along with F to
ing commercial advertisements, a one day broadcast of the            measure the performances of the two summarization meth-
CNN Worldview program lasts for about 22 minutes, and                ods. Let Sman ; Ssum be the set of sentences selected by the
consists of 15 individual news stories on average. The eval-         human evaluator(s), and the summarizer, respectively. The
uation database consists of closed captions of 549 news sto-         standard de nitions of R, P, and F are de ned as follows:
ries whose lengths are in the range of 3 to 105 sentences. As
summarizing short articles does not make much sense in real           R = SmanS Ssum P = SmanS Ssum F = R2RP
                                                                            j          \    j        j      \      j
                                                                                                                              +P
applications, for our evaluations we eliminated all the short                    j   man j               j sum j
stories with less than ten sentences, resulting in 243 docu-         For our evaluations, we set the length of the machine gen-
ments. Table 1 provides the particulars of the evaluation            erated text summaries to the length of the corresponding
database.                                                            manual summaries. When the evaluation is performed using
   Three independent human evaluators were employed to               each individual manual summarization result, both Sman j     j
conduct manual summarization on the 243 documents con-               and Ssum are equal to ve. When the evaluation is per-
                                                                          j        j
tained in the evaluation database. For each document,                formed using the combined result determined by a majority
each evaluator was requested to select exactly ve sentences          vote, Sman becomes variable, and Ssum is set to the value
                                                                            j        j                     j     j
which he/she deemed the most important for summarizing               of Sman .
                                                                        j      j
the story. Because of the disparities in the evaluators' sen-           The evaluation results are shown in Table 3. These re-
tence selections, each document can have between 5 to 15             sults are generated using the BNN weighting scheme (see
                                                                22

                Table 3: Evaluation Results                             2. Inverse document frequency: G(i) = log(N=n(i))
                                                                           where N is the total number of sentences in the docu-
                                                                           ment, and n(i) is the number of sentences that contain
                     First Summarizer Second Summarizer                    term i.
    Test Data         R      P      F        R     P        F
     Assessor 1      0.57   0.60   0.58     0.60  0.62    0.61      When the weighted term-frequency vector Ak of a sentence
     Assessor 2      0.48   0.52   0.50     0.49  0.53    0.51      k is created using one of the above local and global weighting
     Assessor 3      0.55   0.68   0.61     0.55  0.68    0.61      schemes, we further have the choice of
 Majority Vote       0.52   0.59   0.55     0.53  0.61    0.57          1. Normalization: which normalizes Ak by its length Ak .
                                                                                                                             j   j
                                                                        2. No normalization: which uses Ak with its original
Section 4.3 for the detailed descriptions). As evidenced by                form.
the results, despite the very di erent approaches taken by
the two summarizers, their performance measures are quite           Therefore, for creating vector Ak of a sentence k, we have a
compatible. This fact suggests that the two approaches in-          total of 4 2 2 = 16 combinations of the possible weighting
                                                                                 
terpret each other. The rst summarizer (the one using the           schemes. In our experimental evaluations, we have studied
relevance measure) takes the sentence that has the highest          nine common weighting schemes, and their performances are
relevance score with the document as the most important             shown in Figure 1. As seen from the gure, summarizer 1 is
sentence, while the second summarizer (the one based on             less sensitive than summarizer 2 to the changes of weight-
the latent semantic analysis) identi es the most important          ing schemes. Any of the three local weighting schemes (i.e.
sentence as the one that has the largest index value with           Binary, Augmented, logarithm) produces quite similar per-
the most important singular vector. On the other hand, the          formance readings. Adding a global weighing and/or the
 rst summarizer eliminates redundancies by removing all the         vector normalization deteriorates the performance of sum-
terms contained in the selected sentences from the original         marizer 1 by 2 to 3% in average. In contrast, summarizer 2
document, while the second summarizer suppresses redun-             reaches the best performance with the binary local weight-
dancies by using the k th singular vector for the k th round
                         0                            0             ing, no global weighing and no normalization (denoted as
of sentence selection. The rst method is straightforward,           BNN) for most of the cases, while its performance drops a
and it is relatively easy for us to give it a semantic interpre-    bit by adding the global weighing, and deteriorates dramat-
tation. As for the second method, there has been a long his-        ically by adding the normalization into the formula.
tory of arguments about what essentially each of the singular       4.4 Further Observations
vectors represents when a collection of text (which could be
sentences, paragraphs, documents, etc) are projected into              Generic text summarization and its evaluation are very
the singular vector space. Surprisingly, the two di erent           challenging. Because no query nor topic are provided to
methods bring to us very similar summarization outputs.             the summarization task, summarization outputs and per-
This mutual resemblance enhances our belief that each im-           formance judgments tend to lack consensus. In our ex-
portant singular vector does capture a major topic/concept          periments, we have seen the large degree of disparities in
of a document, and two di erent singular vectors do capture         the sentence selections among the three independent eval-
two semantically independent topics/concepts that have the          uators, resulting in lower than expected scores (F=0.55 for
minimum overlap.                                                    summarizer 1, F=0.57 for summarizer 2) from the perfor-
                                                                    mance evaluation by a majority vote. The disparities be-
4.3 Weighting Schemes                                               came even larger with longer documents, and because of
   In our performance evaluations, we studied the in uence          this, we adopted CNN worldview news reports, which have
of di erent weighting schemes on the summarization perfor-          manageable text lengths, in our performance evaluations.
mances as well. As shown by Eq.(1), given a term i, its                On the other hand, for query-relevant text summarization,
weighting scheme is de ned by two parts: local weighting            the most common approach for performance evaluations,
L(i) and global weighting G(i). Local weighting L(i) has            as showcased by the TIPSTER SUMMAC initiative [11],
the following four possible alternatives:                           is that human evaluators use the automatically generated
                                                                    summary to judge the relevance of the original document to
    1. No weight: L(i) = tf (i) where tf (i) is the number of       the user's query. These human evaluators' judgments are
       times term i occurs in the sentence.                         compared with some grand-truth judgments obtained be-
                                                                    forehand, and the accuracy of the human evaluator's judg-
    2. Binary weight: L(i) = 1 if term i appears at least once      ments are then used as the performance measures of the text
       in the sentence; otherwise, L(i) = 0.                        summarizer. A document or a summary is judged relevant if
                                                                    at least one sentence within it is regarded as relevant to the
    3. Augmented weight: L(i) = 0:5 + 0:5 (tf (i)=tf (max))
                                               
                                                                    query. As it is highly probable that a text summarizer can
       where tf (max) is the frequency of the most frequently       extract at least one query-relevant sentence from the origi-
       occurring term in the sentence.                              nal document, this simplistic evaluation method is likely to
    4. Logarithm weight: L(i) = log(1 + tf (i)).                    produce good performance scores.
                                                                       It is observed from Table 3 that the two proposed sum-
Possible global weighting G(i) can be:                              marizers both receive better scores when they are evaluated
                                                                    using the manual summarization results from evaluator 1
    1. No weight: G(i) = 1 for any term i.                          and 3. However, when evaluated using evaluator 2's results,
                                                                 23

           0.7                                                                             0.7
                                                                  Summarizer 1                                                                    Summarizer 1
                                                                  Summarizer 2                                                                    Summarizer 2
           0.6                                                                             0.6
           0.5                                                                             0.5
           0.4                                                                             0.4
 F-Value                                                                         F-Value
           0.3                                                                             0.3
           0.2                                                                             0.2
           0.1                                                                             0.1
            0                                                                               0
                 BNN   BTN   BTC   LNN    LTN   LTC   ANN   ATN      ATC                         BNN   BTN   BTC   LNN    LTN   LTC   ANN   ATN      ATC
                                   Weighting Scheme                                                                Weighting Scheme
                                         (a)                                                                             (b)
           0.7                                                                             0.7
                                                                  Summarizer 1                                                                    Summarizer 1
                                                                  Summarizer 2                                                                    Summarizer 2
           0.6                                                                             0.6
           0.5                                                                             0.5
           0.4                                                                             0.4
 F-Value                                                                         F-Value
           0.3                                                                             0.3
           0.2                                                                             0.2
           0.1                                                                             0.1
            0                                                                               0
                 BNN   BTN   BTC   LNN    LTN   LTC   ANN   ATN      ATC                         BNN   BTN   BTC   LNN    LTN   LTC   ANN   ATN      ATC
                                   Weighting Scheme                                                                Weighting Scheme
                                         (c)                                                                             (d)
Figure 1: The in uence of di erent weighting schemes on the summarization performances. (a),(b),(c),(d):
Evaluation using the manual summarization result from evaluator 1, 2, 3, and the one determined by a
majority vote, respectively. The notation of weighting schemes is the same as the one from the SMART
system. Each weighting scheme is denoted by three letters. The rst, second, and the third letters represent
the local weighting, the global weighing, and the vector normalization, respectively. The meaning of the
letters are as follows: N: No weight, B: Binary, L: Logarithm, A: Augmented, T:Inverse document frequency,
C: Vector normalization.
                                                                                    24

the performance scores drop by 10% in average, dramatically         the two summarizers, they both produced quite compati-
dragging down the performance scores for the evaluation by          ble performance scores. This fact suggests that the two
a majority vote. An in-depth analysis of the cause of this          approaches interpret each other. The evaluations also in-
large di erence has revealed di erent manual summarization          cluded the study of the in uence of di erent VSM weighting
patterns among the three evaluators. Consider the following         schemes on the text summarization performances. Finally,
passage taken from a CNN news story reporting the recent            the causes of the large disparities in the evaluators' manual
Israeli-Palestinian con icts, political e orts for restoring the    summarization results were investigated, and discussions on
calm in the region, and hostile sentiments among Palestinian        human text summarization patterns were provided.
people:                                                                In future work, we plan to investigate machine learning
                                                                    techniques to incorporate additional features for the im-
       (1) ......IN RECENT VIOLENCE MORE                            provement of generic text summarization quality. The addi-
       THAN 90 PEOPLE HAVE BEEN KILLED,                             tional features we are currently considering include linguistic
       THOUSANDS MORE INJURED, THE OVER-                            features such as discourse structure, anaphoric chains, etc,
       WHELMING MAJORITY OF THOSE ARE                               semantic features such as name entities, time, location infor-
       PALESTINIANS.                                                mation, etc. As part of the large-scale video content sum-
       ......                                                       marization project, we also plan to investigate how image
       (2) NOW AFTER A BRIEF LULL IN THE VI-                        and audio acoustic features extracted from video programs
       OLENCE , NEW FIGHTING, NEW CLASHES                           can help to improve the text summarization quality, and
       ERUPTED THURSDAY, AND TONIGHT                                vice versa.
       MORE GUNFIRE REPORTED WITH MORE
       INJURIES OF PALESTINIAN.
       ......                                                       6. REFERENCES
       (3) IN THE NORTHERN WEST BANK                                 [1] M. Sanderson, \Accurate user directed summarization
       TOWN NABLUS , ISRAELI TANKS EX-                                   from existing tools," in Proceedings of the 7'th
       CHANGED FIRE WITH PALESTINIAN                                     International Conference on Information and
       GUNMEN, KILLED AT LEAST 3 OF THEM                                 Knowledge Management (CIKM98), 1998.
       ON WEDNESDAY.                                                 [2] B. Baldwin and T. Morton, \Dynamic
                                                                         coreference-based summarization," in Proceedings of
The above three sentences all cover the topic of Israeli-                the Third Conference on Empirical Methods in Natural
Palestinian con icts. Our two summarizers both selected                  Language Processing (EMNLP3), (Granada, Spain),
sentence (1), and discarded (2) and (3) because of their sim-            June 1998.
ilarities to sentence (1). On the other hand, both evaluator         [3] R. Barzilay and M. Elhadad, \Using lexical chains for
1 and 3 selected sentence (1), while evaluator 2 picked all              text summarization," in Proceedings of the Workshop
the three sentences for summarizing the topic. This example              on Intelligent Scalable Text Summarization, (Madrid,
represents a typical pattern that happens repeatedly in the              Spain), Aug. 1997.
whole evaluation process. The fact suggested by this phe-            [4] C. Buckley and et al., \The smart/empire tipster ir
nomenon is that, to summarize a document, some people                    system," in Proceedings of TIPSTER Phase III
strive to select sentences that maximize the coverage of the             Workshop, 1999.
document's main content, while others tend to rst deter-
mine the most important topic of the document, and then              [5] J. Goldstain, M. kantrowitz, V. Mittal, and
collect only the sentences that are relevant to this topic. Ev-          J. Carbonell, \Summarizing text documents: Sentence
idently, when it comes to the evaluation of our two proposed             selection and evaluation metrics," in Proceedings of
summarization methods, the former type of evaluators gen-                ACM SIGIR'99, (Berkeley, CA), Aug. 1999.
erates a higher accuracy score than the latter.                      [6] E. Hovy and C. Lin, \Automated text summarization
                                                                         in summarist," in Proceedings of the TIPSTER
5.     SUMMARIES                                                         Workshop, (Baltimore, MD), 1998.
   This paper presented two text summarization methods               [7] http://www.SRA.com.
that create generic text summaries by ranking and extract-           [8] W. Press and et al., Numerical Recipes in C: The Art
ing sentences from the original documents. The rst method                of Scienti c Computing. Cambridge, England:
uses standard IR methods to rank sentence relevances, while              Cambridge University Press, 2 ed., 1992.
the second method uses the latent semantic analysis tech-            [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer,
nique to identify semantically important sentences, for sum-             and R. Harshman, \Indexing by latent semantic
mary creations. Both methods strive to select sentences that             analysis," Journal of the American Society for
are highly ranked and di erent from each other. This is an               Information Science, vol. 41, pp. 391{407, 1990.
attempt to create a summary with a wider coverage of the            [10] M. Berry, S. Dumais, and G. O'Brien, \Using linear
document's content and a less redundancy. For experimen-                 algebra for intelligent information retrieval," Tech.
tal evaluations, a database consisting of two months of the              Rep. UT-CS-94-270, University of Tennessess,
CNN Worldview news programs was constructed, and per-                    Computer Science Department, Dec. 1994.
formances of the two summarization methods were evalu-              [11] T. Firmin and B. Sundheim, \Tipster/summac
ated by comparing the machine generated summaries with                   summarization analysis participant results," in
the manual summaries created by three independent human                  TIPSTER Text Phase III Workshop, 1998.
evaluators. Despite the very di erent approaches taken by
                                                                 25

