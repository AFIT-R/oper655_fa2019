---
title: "Document Summarization"
author: "Trey Pujats Clarence Williams and Maria Schroeder"
date: "11/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Document Summarization

Document summarization's ultimate goal is to identify the most important and mostfrequent text within a document and ouput those results, capturing the best summary of the document. Document summarization can be categorized as either abstractive or extractive. Abstractive document summarization uses semantic relationships between text in the original document and uses natural language processing to generate new text to describe the summarization of the document. Abstractive summarization is generally the way that humans create summaries. Typically document summarization techniques do not use this kind of summarizationbecause problems of semantic representation and others are difficult and currently not easy to deal with. In contrast, extractive ranks the text within the document against one another and then extracts the most important text/ highest ranking textin the document. It chooses a subset of sentences to output verbatim from the original text as the summary. In general the steps of Document Summarization can be summarized as follows:

1.Construct an intermediate representation of the document

2. Score the sentences based on the representation

3. Select a summary of the text using the sentences.



The most common is the text ranking method. The text ranking method breaks the document into text strings of the users choice and measures similarity between these text stuructures (consider them sentences for the purpose of this presentation). Graphically, these sentences create vertices and the frequency of similarity of sentences is measured by edges that connect to vertices within the graph. Furthermore, the edges can have weight to them based on how important the vertex is that is connected to the edge. A vertex of high importance is meaured by its similarity to other vertices and the similarity is measured by lexical or semantic relations to the other text. Rada Mihalcea shows the four steps to perform text ranking in document summarization:

1. Identify text units that best define the task at hand,and add them as vertices in the graph.

2. Identify relations that connect such text units, anduse these relations to draw edges between verticesin the graph. Edges can be directed or undirected,weighted or unweighted.

3. Iterate the graph-based ranking algorithm until convergence.

4. Sort vertices based on their final score. Use the values attached to each vertex for ranking/selection decisions.There are many different ways to score sentences. Methods of scoring use location of words, words in titles, indicator phrases, and/or cue methods. The use of location assumes greater importance to sentences at the beginning and end of sentences. The use of titles heuristics assume a higher importance of sentences that contain keywords from titles. There are certain words that imply or accompany words that help summarize a given report. An example of this may be the phrase "To conclude". Additionally, cue words can either imply importance of a sentence or its lack of importance. An example of a word that implies greater imporance is "significant". A word that may imply lesser importance is "hardly". Based off of these methods, sentences can be scored from these heuristics. In our dataset, sincethere are no subsections and it is not a single large document some of these may not be helpful.

Practical Applications:

One practical application of document summarization is used by Google to summarize webpages.
Generate abstracts for documents.

```{r, Overview of Document Summarization}

knitr::include_graphics("C:/Users/treyp/OneDrive/Documents/OPER 655 - Text Mining/oper655_fa2019/student_project_folders/oper655_fa2019_pujats/3.png")

```



## Cleaning the Data

Since Captain Brandon Hufstetler cleaned the code for text sentiment, we did not duplicate efforts although we alterred thim to fit the document summarization setup. Instead, we did not unnest each word but kept each review together. This made it easier to categorize the data and evaluate complete sentences rather than words for sentiment. The finished dataset is review_tidy and it is shown below. Next we decided to look at each phone separate from one another. Summaries of different phones from different companies does not capture anything useful to the consumer or the producer in this case. Subsets were created to split the data by phone including the iPhone4, iPhone5, iPhone6, iPhone7, galaxy S5, galaxy s6, galaxy s7, and galaxy s8. Using these subsets, we performed document summarization.
```{r, Cleaning the data}
pacman::p_load(tidyr,
               tidytext,
               tidyverse,
               textdata,
               dplyr,
               stringr,
               ggplot2,
               magrittr,
               wordcloud,
               reshape2,
               textmineR,
               LSAfun,
               igraph,
               textrank,
               ggraph,
               lattice,
               udpipe)



root <- rprojroot::find_root(rprojroot::is_rstudio_project)
file_loc <- file.path(root,"data","phone_user_reviews")

file_list <- list.files(path = file_loc,
                        pattern = "",
                        full.names = TRUE)
reviews_tidy <- tibble::tibble()
manu_pattern <- "/cellphones/[a-z0-9]+"
prod_pattern <- paste(manu_pattern, "-|/", sep = "")
for (i in file_list){
  input <- load(i,ex <- new.env())
  text_raw <- get(ls(ex),ex)
  text_en <- text_raw[text_raw$lang=="en",]
  rm(ex, text_raw, input, i)
  
  clean <- tibble::tibble(score = text_en$score,
                          maxscore = text_en$score_max,
                          text = text_en$extract,
                          product = gsub(prod_pattern, "", text_en$phone_url),
                          author = text_en$author,
                          manufacturer = gsub("/cellphones/","",str_extract(text_en$phone_url,manu_pattern))) 
  
  reviews_tidy <- base::rbind(reviews_tidy, clean)
  rm(text_en, clean)
}
rm(file_list, root, manu_pattern, prod_pattern, file_loc)
head(reviews_tidy)


GalaxyS5<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s5"))
GalaxyS6<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s6"))
GalaxyS7<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s7-edge"))
GalaxyS8<-na.omit(subset(reviews_tidy, reviews_tidy$product=="galaxy-s8"))

iPhone4<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-4"))
iPhone5<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-5"))
iPhone6<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-6"))
iPhone7<-na.omit(subset(reviews_tidy, reviews_tidy$product=="iphone-7"))

WorstReviews<-na.omit(subset(reviews_tidy, reviews_tidy$score<2))
BestReviews<-na.omit(subset(reviews_tidy, reviews_tidy$score>9))


```

## Cell Phone Summarization

After cleaning the data we implemented document summarization into the cell phone data. There are a number of ways to do this and some have more tunable paramters than others. We used a few methods to see how results differed or were alike, but some results gave errors based on singularity in the dataset. Using "textrank_sentences" we were able to extract summarizations for all of the documents. However, we did limit ourselves to choose a random sample of 100 reviews due to computational power. The text rank goes through each sentence and draws comparisons by each sentence and even by each word, so the larger the document you work with, the much longer it will take to analyze.

```{r, Galaxy S5}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = GalaxyS5$text[1:100]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

genericSummary(GalaxyS5$text[1:100],3)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'Galaxy S5',
       caption = "Source: Oper 655 - Text Mining")


```

```{r, Galaxy S6}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = GalaxyS6$text[sample(nrow(GalaxyS6), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'Galaxy S6',
       caption = "Source: Oper 655 - Text Mining")


```



```{r, Galaxy S7}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = GalaxyS7$text[sample(nrow(GalaxyS7), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'Galaxy S7',
       caption = "Source: Oper 655 - Text Mining")


```



```{r, Galaxy S8}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = GalaxyS8$text[sample(nrow(GalaxyS8), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'Galaxy S8',
       caption = "Source: Oper 655 - Text Mining")


```



```{r, iPhone 4}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = iPhone4$text[sample(nrow(iPhone4), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'iPhone 4',
       caption = "Source: Oper 655 - Text Mining")


```


```{r, iPhone 5}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = iPhone5$text[sample(nrow(iPhone5), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'iPhone5',
       caption = "Source: Oper 655 - Text Mining")


```


```{r, iPhone 6}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = iPhone6$text[sample(nrow(iPhone6), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'iPhone 6',
       caption = "Source: Oper 655 - Text Mining")


```


```{r, iPhone 7}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = iPhone7$text[sample(nrow(iPhone7), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'iPhone 7',
       caption = "Source: Oper 655 - Text Mining")


```


```{r, Best Reviews}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = BestReviews$text[sample(nrow(BestReviews), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'Best Reviews',
       caption = "Source: Oper 655 - Text Mining")


```





```{r, Worst Reviews}

#This uses tidy text to create tokens of sentences anf of words from the document. This is necessary to analyze the similarity between sentences, get rid of stop words and to rank the sentences.
article_sentences <- tibble(text = WorstReviews$text[sample(nrow(WorstReviews), 100)]) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)

article_words <- article_sentences %>%
  unnest_tokens(word, sentence)

article_words <- article_words %>%
  anti_join(stop_words, by = "word")

article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)


#This shows us the top three sentences that summarize the document
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%
  pull(sentence)

#This examines the lowest rank of sentences that explain the reviews.
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)

#This shows where, within the document, you see the most important text.
article_summary[["sentences"]] %>%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  labs(x = "Sentence",
       y = "TextRank score",
       title = "Location within the data where most informative text occurs",
       subtitle = 'Worst Reviews',
       caption = "Source: Oper 655 - Text Mining")


```


## Keyword Extraction

Keyword extraction is a helpful tool to documetn summarization. If you have a very large document and your summarization turns out to be a few pages in length still, keywords could help identify what you need to look for within the summary as well. There are a number of ways to extract keywords, either singularly or as bigrams. We begin by annotating the text, using "udpipe". This tool allows us to quickly go through all of the text and annotate each word shown in x. We then get a quick idea of the most common words used. This allows us to see what words we can expect and the most common words in our document. Here we specifically look at nouns, which are more common to be keywords compared to adjectives and verbs. 

```{r, creating the corpus for analysis}

library(udpipe)
library(textrank)
## First step: Take the English udpipe model and annotate the text.
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = GalaxyS5$text)
x <- as.data.frame(x)
head(x)

#Lemma is just the column of all of the words in the document
#Here we simply show the frequency of the words.
stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)
library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "orange", main = "Most occurring nouns", xlab = "Freq")


```


Next we may want to look at expressions, since singular words can be misleading out of context. Allowing for expressions could give a better sense of what the documetn is about. We check for concurrences where the two words are directly next to each other, in the same sentence, or a few words away from each other. 

```{r, multiword phrases and expressions}

## Collocation (words following one another)
stats <- keywords_collocation(x = x, 
                              term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                              ngram_max = 4)
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")), 
                      term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma, 
                      relevant = x$upos %in% c("NOUN", "ADJ"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma, 
                      relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(stats)

#From here we build out a word network showing how closely related and used together multi word phrases are in the document. 
wordnetwork <- head(stats, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  theme(legend.position = "none") +
  labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")

```




Text ranking is another useful tool to understand the document. This is very similar to determining the highest rank of sentences in document summarization, but this is applied to multiword expressions instead. The word cloud shows the most frequently occuring phrases, only the top 200 expressions.

```{r, textrank}

stats <- textrank_keywords(x$lemma, 
                           relevant = x$upos %in% c("NOUN", "ADJ"), 
                           ngram_max = 8, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 5)
library(wordcloud)
wordcloud(words = stats$keyword, freq = stats$freq, max.words = 200)

```



Rapid automatic keyword extraction (RAKE) is very similar to text ranking but it also adds an element of tfidf to its calculations. It still ranks the expressions based on how many times it occurs with other words vs the frequency of occurences in the document. From out data, we can see that it did not provide much help, but given other datasets it could be beneficial to see certain phrases that are mentioned significantly together and not separatlely. It is obvious that in our dataset, we expect to see "Samsung Galaxy" right next to each other and not apart, unless we begin to speak about constellations in our galaxy.

```{r, RAKE}

stats <- keywords_rake(x = x, 
                       term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                       relevant = x$upos %in% c("NOUN", "ADJ"),
                       ngram_max = 4)
head(subset(stats, freq > 3))

```

Again we pull out phrases as oppose to multiword expressions, this is not including verbs, nouns and adjectives.

```{r, Phrases}
## Simple noun phrases (a adjective+noun, pre/postposition, optional determiner and another adjective+noun)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = x$token, 
                          pattern = "(A|N)+N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, ngram_max = 4, detailed = FALSE)
head(subset(stats, ngram > 2))
```


Lastly, we identify the most common phrases in the dataset to get a feel for the overall docuement.

```{r}

stats <- merge(x, x, 
               by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
               by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
               all.x = TRUE, all.y = FALSE, 
               suffixes = c("", "_parent"), sort = FALSE)
stats <- subset(stats, dep_rel %in% "nsubj" & upos %in% c("NOUN") & upos_parent %in% c("ADJ"))
stats$term <- paste(stats$lemma_parent, stats$lemma, sep = " ")
stats <- txt_freq(stats$term)
library(wordcloud)
wordcloud(words = stats$key, freq = stats$freq, min.freq = 3, max.words = 100,
          random.order = FALSE, colors = brewer.pal(6, "Dark2"))


```







Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
