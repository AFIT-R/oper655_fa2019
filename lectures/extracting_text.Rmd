---
title: "Importing Text From Documents"
author: "Jason Freels"
date: "10/9/2018"
output: 
  html_document:
    toc: yes
    toc_float: yes
    css: 'css/logm655.css'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```


## Overview

This document walks through the process of extracting text data from documents.  It is assumed that the documents from which the data will be extracted are saved using on of the following formats:

1. Plain text - i.e. notepad `.txt` files or a similar format
2. Word Document
3. PDF document - saved as an actual file and NOT as a scanned image

There are a number of R packages and/or 3^rd^ party tools that can be used for this purpose.  I'll demo these tools by extracting the text from the `supreme_court_opinions_2017_sample` folder, note that all of these files are saved using the PDF format.

## Extracting textual data from PDF documents

The documents from which the text will be extracted are the United States Supreme Court Opinions from 2017.  This documents can be accessed from [**here**](https://www.supremecourt.gov/opinions/slipopinion/17).  There are a total of 85 documents in this corpus.  Rather than download each of these documents separately, I prefer to let R do this for me, you can too by using the code shown in the next section.  First, install and load the necessary packages.  The `pacman` package is a helpful package for simplifying the process of getting packages, let's install it first.

```{r, eval=FALSE}
install.packages('pacman')
```

With the `pacman` package installed, we can easily install and load other packages we'll need to download the files from the web.

```{r, eval=FALSE}
pacman::p_load(XML,
               rvest,
               RCurl,
               rprojroot)
```

### Saving local copies of the documents 

With these packages installed, we can now download the documents from the url using the code shown below.  <u>**However, do not run this code as it is written in the LOGM655 project - it will overwrite the existing files.  I'll show you what to change**</u>

```{r, eval=FALSE}
# Assign the URL to a text string
url_root  <- 'https://www.supremecourt.gov'
     url  <- 'https://www.supremecourt.gov/opinions/slipopinion/17'
     
# Assign the root of the project this
# helps locate where to save the files
# Before going forward you should change 
# these values to a location on your machine
proj_root   <- find_root(is_rstudio_project)
save_folder <- file.path(proj_root,'raw_data_files','supreme_court_opinions_2017')

# Extract html/xml content from URL
rcurl.doc <- RCurl::getURL(url, 
                           .opts = RCurl::curlOptions(followlocation = TRUE))

# Parse html content 
url_parsed <- XML::htmlParse(rcurl.doc, asText = TRUE)

# We need to get the href attributes from 
# the anchor tags <a> stored in the table 
# as table data tags <td>
# First, let's get all of the attributes
attrs <- XML::xpathApply(url_parsed, "//td//a", XML::xmlAttrs)

# Next, we'll split out the hrefs 
# from the other attributes
hrefs <- sapply(seq_along(attrs), FUN = function(x) attrs[[x]][['href']])

# Then, we only want the hrefs for the files 
# that have a .pdf file extension
pdfs  <- hrefs[tools::file_ext(hrefs) == 'pdf']

# Construct a list of URL's for each file
# by pasting two character strings together  
files <- paste0(url_root,pdfs)

# loop through each element in the files
# vector and download the file to destfile
for(i in files) {
  
  filename <- basename(i)
  download.file(i, 
                destfile = file.path(save_folder,filename),
                method = 'curl')
  
}
```

Now that we have these document downloaded locally, I'm going to use a small set of these documents `supreme_court_opinions_2017_sample` folder. Before we get too far, let's install some helpful packages.

```{r}
pacman::p_load(qdapTools,
               pdftools,
               antiword,
               glue)
```

### Using the XpdfReader

First, let's use XpdfReader by letting R talk to our system (i.e. command prompt or terminal).  Xpdf is an open source project developed by [**Glyph & Cog**](http://www.glyphandcog.com/) for viewing Portable Document Format (PDF)
files.  The Xpdf project also includes **xpdf tools** which contain the following utilities that are useful for extracting data from pdf files:

- pdftotext - Convert a PDF file to text
- pdftohtml - Convert a PDF file to HTML
- pdfinfo - Dump a PDF file's Info dictionary (plus some other useful information)
- pdffonts - List the fonts used in a PDF file and various information for each font
- pdfdetach - List or extract embedded files (attachments) from a PDF file
- pdftoppm - Convert a PDF file to a series of PPM/PGM/PBM-format bitmaps
- pdftopng - Convert a PDF file to a series of PNG image files
- pdfimages - Extract the images from a PDF file

To use these utilities we must first download xpdf and xpdf tools from [**this site**](http://www.xpdfreader.com/).  After downloading and unzipping xpdf tools, make sure to note the file location where it was saved.  On my machine, the main xpdf folder is located at

```{r}
xpdf_tools <- 'C:/Program Files/xpdf-tools-win-4.00/bin64'
```

Next we'll create a character vector containing the names of the files in the `supreme_court_opinions_2017_sample` folder.

```{r}
library(rprojroot)
root <- find_root(is_rstudio_project)

dest <- file.path(root, 'raw_data_files', 'supreme_court_opinions_2017_sample')

# make a vector of PDF file names
my_files <- list.files(path = dest, 
                       pattern = "pdf",  
                       full.names = TRUE)

my_files
```

Now, let's use the `pdftotext` utility to extract the text from the first document in `my_files`.  The `pdftotext` utility converts each PDF file in into a text file.  By default, the text file is created in the same directory as the PDF file.  Since this is a command-line utility we need to construct the commands to send, normally this would look like the code below.

```{r}
if(nchar(Sys.which('pdftotext') > 0)) {
  
   system('pdftotext')

}
```

To have R call pdftotext, we need to paste (or glue) these four separate charater strings together into a single command.  This can be done as shown below. 

```{r}
cmd1 <- 'pdftotext' # which utility are we calling?
cmd2 <- ''          # which options? - here we use none
cmd3 <- my_files[1] # which file to convert
cmd4 <- ''          # which file to write to

# Two options to connect the strings
CMD1 <- glue::glue("{cmd1} {cmd2} {cmd3} {cmd4}")
CMD2 <- paste(cmd1, cmd2, cmd3, cmd4, sep = ' ')
```

Now, we send this command to either the command prompt or terminal, depending on the type of OS being used.  

```{r, eval=FALSE}
system(CMD1)
```

To have `pdftotext` do this action recursively for each of the files we can run the above command in a loop or use one of the apply functions, in this case `lapply()` 

```{r}
lapply(my_files, 
       FUN = function(x) system(glue::glue("pdftotext {x}"), wait = FALSE))
```

Now that the data has been extracted into a text file it can then be read into R as a character vector using the `readLines()` function. Note that each element in character vector is a line of text.  A line of text in defined by a certain number of characters, this number of characters may not coincide with the number of characters on the original pdf document.  If we desire to maintain the exact layout of the document we can specify the option `-layout` to maintain the original layout of the text.  

```{r}
text_files <- list.files(path = dest, 
                         pattern = "txt",  
                         full.names = TRUE)

text1 <- readLines(con = text_files[1])
text1[1:50]
```

The remaining utilities in xpdf tools can be used in a similar manner as to what what shown here for `pdftotext`.

### Using the `pdftools` package

The pdftools package is an R interface to the Poppler C++ API. The package makes it easy to use several utilities based on 'libpoppler' for extracting text, fonts, attachments and metadata from a PDF file. The package also supports high quality rendering of PDF documents info PNG, JPEG, TIFF format, or into raw bitmap vectors for further processing in R. The `pdftools` contains the following user-level functions 

```{r}
data.frame(function_names = getNamespaceExports('pdftools'))
```

that can be used to retreive information in a similar fashion as to what was shown for the xpdf tools.  As example using the `pdf_text()` function extracts the text from a PDF file as a character vector. Note that in this case each character vector is an entire page. 

```{r}
text2 <- pdftools::pdf_text(pdf = my_files[1])
text2[1]
```

The `pdftools` package does not contain a function to save the text directly to a text file, however we can write to a text file using the `writeLines()` function. A downside to this is that we must keep track of the name of the original document when specifying a name to the new `.txt` file. This could lead to errors in assigning the wrong name to a file. Also, note that the output file contains a blank line between each line of text.  This may not present a problem depending on the desired output, however it presents an extra step in our data preparation process.

```{r eval=FALSE}
writeLines(text = text2,
           con = gsub('pdf','txt',file.path(dest,basename(my_files[1]))))
```

## Extracting textual data from MS^&#174;^ Word documents

Once again, we need to download the Word files from the web, which can be done using the code below. <u>**Again, do not run this code as it is written in the LOGM655 project - it will overwrite the existing files.  I'll show you what to change**</u>

```{r, eval=FALSE}
# Assign the URL to a text string
url_root  <- 'http://hhoppe.com/'
     url  <- 'http://hhoppe.com/microsoft_word_examples.html'
     
# Assign the root of the project this
# helps locate where to save the files
# Before going forward you should change 
# these values to a location on your machine
proj_root   <- find_root(is_rstudio_project)
save_folder <- file.path(proj_root,'raw_data_files','msword_document_examples')

# Extract html/xml content from URL
rcurl.doc <- RCurl::getURL(url, 
                           .opts = RCurl::curlOptions(followlocation = TRUE))

# Parse html content 
url_parsed <- XML::htmlParse(rcurl.doc, asText = TRUE)

# We need to get the href attributes from 
# the anchor tags <a> stored on the page 
attrs <- XML::xpathApply(url_parsed, "//a", XML::xmlAttrs)

# Next, we'll split out the hrefs 
# from the other attributes
hrefs <- sapply(seq_along(attrs), FUN = function(x) attrs[[x]][['href']])

# Then, we only want the hrefs for the files 
# that have a .docx file extension
docx  <- hrefs[tools::file_ext(hrefs) == 'docx']

# Construct a list of URL's for each file
# by pasting two character strings together  
files <- paste0(url_root, docx)

# loop through each element in the files
# vector and download the file to destfile
for(i in files) {
  
  filename <- basename(i)
  download.file(i, 
                destfile = file.path(save_folder,filename),
                method = 'curl')
  
}
```

### Using the `qdapTools` package

The `qdapTools` package is a collection of tools associated with the `qdap` package that may be useful outside of the context of text analysis. In the R language there are 4 packages in the `qdap` family used for qualitative data analysis

- `qdap` - Bridging the Gap Between Qualitative Data and Quantitative Analysis
- `qdapTools` - Tools for the 'qdap' Package
- `qdapRegex` - Regular Expression Removal, Extraction, and Replacement Tools
- `qdapDictionaries` - Dictionaries and Word Lists for the 'qdap' Package

Within the `qdapTools` package is the `read_docx()` function that is used for (you'll never guess) reading `.docx` files.  First, let's list the files like we did for the PDF files before.

```{r}
library(rprojroot)
root <- find_root(is_rstudio_project)

dest <- file.path(root, 'raw_data_files', 'msword_document_examples')

# make a vector of PDF file names
ms_files <- list.files(path = dest, 
                       pattern = "docx",  
                       full.names = TRUE)

ms_files
```

Now, let's read in the content of the first file in the `ms_files` vector which is <u>`r ms_files[1]`</u>.

```{r}
docx1 <- qdapTools::read_docx(file = ms_files[1])
docx1[90]
```

Here we see that this function extracts the text as paragraphs, rather than as lines or pages.

## Other Datasets

There are many other sources of publicly available text data for use in training NLP models or to test out some of the basic NLP tasks.  A few sources of such data are listed below.  

1. [**kaggle**](https://www.kaggle.com/datasets?sortBy=relevance&group=all&search=text)
2. [**UCSD**](https://ucsd.libguides.com/data-statistics/textmining)
3. [**QUANDL**](https://www.researchgate.net/deref/https%3A%2F%2Fwww.quandl.com%2F) 
4. [**kdnuggets**](https://www.kdnuggets.com/datasets/index.html) 
5. [**Amazon reviews**](https://snap.stanford.edu/data/web-Amazon.html) 
6. [**ENRON emails**](https://www.cs.cmu.edu/~./enron/) 
7. [**Hillary Clinton's declassified emails**](http://www.readhillarysemail.com)
8. [**R package: Harry Potter**](https://github.com/bradleyboehmke/harrypotter)