---
title: "Untitled"
author: "Jason Freels"
date: "8/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Working with the Sarcasm data set

```{r}
library(reticulate)
reticulate::conda_python(envname = "r-reticulate")
```

```{python}
import json
import tensorflow as tf

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

Set up parameters

```{python}
vocab_size = 5000 # constrain the size of our vocabulary
embedding_dim = 16 # dont remember
max_length = 24    # Chunk the text in 32-word segments
trunc_type = "post" # truncate sentences longer that 32 words starting at the end
padding_type = "post" # pad the sequence matrix at the end, not the beginning
oov_tok = "<OOV>"     # symbol to use for words that are not in our vocab
training_size = 20000 # subset data to use the first 20000 records
```

```{r, cache=T}
sarcasm <- jsonlite::read_json("https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json")
```

```{python}
sentences = []
labels = []

for item in r.sarcasm:
  sentences.append(item["headline"])
  labels.append(item["is_sarcastic"])
```

```{python}
training_sentences = sentences[0:training_size]
testing_sentences = sentences[training_size:]
training_labels = labels[0:training_size]
testing_labels = labels[training_size:]
```

```{python}
# Build the tokenizer object with desired parameters
tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)

# Call the function `fit_on_texts()` within the tokenizer object
# apply the function to  the training sentences to create
# a word index (vocabulary) as a sub-object within tokenizer  
tokenizer.fit_on_texts(training_sentences)

# extract the word index from tokenizer as a separate object
word_index = tokenizer.word_index
```

```{python}
# Call the function `texts_to_sequences()` within the tokenizer object
# apply the function to `training sentences` to convert the words to numbers
training_sequences = tokenizer.texts_to_sequences(training_sentences)

# Call the tensorflow function `pad_sequences()`
# Apply the function to `training_sequences` to generate the matrix 
# of embeddings
training_padded = pad_sequences(training_sequences, 
                                maxlen = max_length,
                                padding = padding_type,
                                truncating = trunc_type)
```


<!--
import csv
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

!wget --no-check-certificate \
    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \
    -O /tmp/bbc-text.csv

vocab_size = 10000
embedding_dim = 16 # YOUR CODE HERE
max_length = 120 # YOUR CODE HERE
trunc_type = "post"# YOUR CODE HERE
padding_type = "post"# YOUR CODE HERE
oov_tok = "<OOV>"# YOUR CODE HERE
training_portion = .8

sentences = []
labels = []
stopwords = [ "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during", "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves" ]
print(len(stopwords))
# Expected Output
# 153

with open("/tmp/bbc-text.csv", 'r') as csvfile:
    stu = csv.reader(csvfile)
    i = next(stu)
    for row in stu:
      labels.append(row[0])
      sentences.append(row[1])

print(i)
print(len(labels))
print(len(sentences))
print(sentences[2])
# Expected Output
# 2225
# 2225
# tv future hands viewers home theatre systems  plasma high-definition tvs  digital video recorders moving living room  way people watch tv will radically different five years  time.  according expert panel gathered annual consumer electronics show las vegas discuss new technologies will impact one favourite pastimes. us leading trend  programmes content will delivered viewers via home networks  cable  satellite  telecoms companies  broadband service providers front rooms portable devices.  one talked-about technologies ces digital personal video recorders (dvr pvr). set-top boxes  like us s tivo uk s sky+ system  allow people record  store  play  pause forward wind tv programmes want.  essentially  technology allows much personalised tv. also built-in high-definition tv sets  big business japan us  slower take off europe lack high-definition programming. not can people forward wind adverts  can also forget abiding network channel schedules  putting together a-la-carte entertainment. us networks cable satellite companies worried means terms advertising revenues well  brand identity  viewer loyalty channels. although us leads technology moment  also concern raised europe  particularly growing uptake services like sky+.  happens today  will see nine months years  time uk   adam hume  bbc broadcast s futurologist told bbc news website. likes bbc  no issues lost advertising revenue yet. pressing issue moment commercial uk broadcasters  brand loyalty important everyone.  will talking content brands rather network brands   said tim hanlon  brand communications firm starcom mediavest.  reality broadband connections  anybody can producer content.  added:  challenge now hard promote programme much choice.   means  said stacey jolna  senior vice president tv guide tv group  way people find content want watch simplified tv viewers. means networks  us terms  channels take leaf google s book search engine future  instead scheduler help people find want watch. kind channel model might work younger ipod generation used taking control gadgets play them. might not suit everyone  panel recognised. older generations comfortable familiar schedules channel brands know getting. perhaps not want much choice put hands  mr hanlon suggested.  end  kids just diapers pushing buttons already - everything possible available   said mr hanlon.  ultimately  consumer will tell market want.   50 000 new gadgets technologies showcased ces  many enhancing tv-watching experience. high-definition tv sets everywhere many new models lcd (liquid crystal display) tvs launched dvr capability built  instead external boxes. one example launched show humax s 26-inch lcd tv 80-hour tivo dvr dvd recorder. one us s biggest satellite tv companies  directtv  even launched branded dvr show 100-hours recording capability  instant replay  search function. set can pause rewind tv 90 hours. microsoft chief bill gates announced pre-show keynote speech partnership tivo  called tivotogo  means people can play recorded programmes windows pcs mobile devices. reflect increasing trend freeing multimedia people can watch want  want.

train_size = int(len(sentences) * training_portion) # YOUR CODE HERE

train_sentences = sentences[0:train_size]# YOUR CODE HERE
train_labels = labels[0:train_size]# YOUR CODE HERE

validation_sentences = sentences[train_size:] # YOUR CODE HERE
validation_labels =labels[train_size:] # YOUR CODE HERE

print(train_size)
print(len(train_sentences))
print(len(train_labels))
print(len(validation_sentences))
print(len(validation_labels))

# Expected output (if training_portion=.8)
# 1780
# 1780
# 1780
# 445
# 445

tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)# YOUR CODE HERE
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index # YOUR CODE HERE

train_sequences = tokenizer.texts_to_sequences(train_sentences)# YOUR CODE HERE
train_padded = pad_sequences(train_sequences, 
                             maxlen = max_length,
                             padding = padding_type,
                             truncating = trunc_type)

print(len(train_sequences[0]))
print(len(train_padded[0]))

print(len(train_sequences[1]))
print(len(train_padded[1]))

print(len(train_sequences[10]))
print(len(train_padded[10]))

# Expected Ouput
# 449
# 120
# 200
# 120
# 192
# 120

validation_sequences = tokenizer.texts_to_sequences(validation_sentences)
validation_padded = pad_sequences(validation_sequences, 
                               maxlen = max_length,
                               padding = padding_type,
                               truncating = trunc_type)

print(len(validation_sequences))
print(validation_padded.shape)

# Expected output
# 445
# (445, 120)

label_tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)
label_tokenizer.fit_on_texts(train_labels)

training_label_seq = tokenizer.texts_to_sequences(train_labels)
validation_label_seq = tokenizer.texts_to_sequences(validation_labels)

print(training_label_seq[0])
print(training_label_seq[1])
print(training_label_seq[2])
print(training_label_seq.shape)

print(validation_label_seq[0])
print(validation_label_seq[1])
print(validation_label_seq[2])
print(validation_label_seq.shape)

# Expected output
# [4]
# [2]
# [1]
# (1780, 1)
# [5]
# [4]
# [3]
# (445, 1)
-->

```{python}
# Do the same as above for `testing_sentences`
testing_sequences = tokenizer.texts_to_sequences(testing_sentences)

testing_padded = pad_sequences(testing_sequences, 
                               maxlen = max_length,
                               padding = padding_type,
                               truncating = trunc_type)
```

Create the neural network

```{python}
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(24, activation="relu"),
  tf.keras.layers.Dense(1, activation="sigmoid")
])

model.compile(loss="binary_crossentropy", optimizer="adam",metrics= ["accuracy"])

model.summary()
```

```{python}
num_epochs = 30

history = model.fit(x = training_padded,
                    y = training_labels,
                    epochs = num_epochs,
                    validation_data = (testing_padded,testing_labels),
                    verbose = 2)
```

```{python}
import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history["val_"+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, "val_"+string])
  plt.show()
```

```{python}
plot_graphs(history, "acc")
```

```{python}
plot_graphs(history, "loss")
```
