---
output: html_document
---

```{r}
library(reticulate)
reticulate::conda_python(envname = "r-reticulate")
```

Last time, we tokenized the text in a corpus, transforming the words into number sequences wherein the numbers represented the values in a `key : value` pair and the 
keys represented the word. Now, we're going to build a tensorflow model to look at the sentiment of labeled training text and then use that trained understanding to classify some new text.

You may be wondering, how can we get sentiment from a string of numbers representing words. Turns out for a classification model we don't need the actual sentiment scores like what we determined earlier in the course.  We just need to a model that can differentiate the sentiment in one class from the sentiment in the other class.  This is something that can be learned from a corpus of words, using a process called `embedding`. The idea behind embedding is that associated words are clustered as vectors in a multi-dimensional space. 

To understand this we'll use the well known IMDB data set.  This data set contains movie reviews partitioned into two categories: positive review and negative reviews. Using the text from the reviews in a set of training data, along with the assigned class labels, we'll be able to build these embeddings showing a clear clustering
of words that are distinct to both of these review types. 

## The <a target=" " href="http://ai.stanford.edu/~amaas/data/sentiment/">`IMDB`</a> dataset

To access the data from in a form that is amenable to `tensorflow` we will use the `tensorflow_datasets` module. This module is included in the `conda` repository and can be installed using the Anaconda Navigator, from within R using

```{r, eval=FALSE}
reticulate::conda_install(packages = "tensorflow-datasets")
```

or from the Anaconda prompt using

```{asis}
conda install tensorflow-datasets
```

A number of data sets have been made available as part of the `tensorflow_datasets` module, these data sets can be viewed from <a target=" " href="https://www.tensorflow.org/datasets/catalog/overview">**this site**</a>.

To download the data, we first need to import the module into our working environment 

```{python}
import tensorflow_datasets as tf
```

Now, we can load the data using the `load()` method from within the `tensorflow_datasets` module as shown in the chunk below.  Note, that this method will search `data_dir` to check whether an `imdb_reviews` directory already exists.  If this directory is not found it will be created and the data will be downloaded into it and then loaded into the current working environment.  If the directory is found the data will simply be loaded. If `data_dir` is undefined this method will attempt to download the data to a default directory.  In my case, my Virus protection would not allow these new directories/files to be created in a "protected folder".  Therefore, it may be wise to specify a location. Note that this directory may be defined using a relative path. 

The `with_info = True` argument indicates that the metadata will also be included in the download.  The `as_supervised = True` argument indicated the structure of the returned `tf.data.Dataset`.  If `True` the data will have a 2-tuple structure `(input, label)`, if `False` the data be returned as a dictionary with all the features. The `shuffle_files = False` indicates whether to shuffle the input files -- this argument was added to prevent a warning message that kept popping up for me. 

```{python}
imdb, info = tf.load("imdb_reviews", 
                        with_info = True, 
                        as_supervised = True,
                        data_dir = "data",
                        shuffle_files = False)
```

Now that we've downloaded the `imdb` and `info` data objects, let's take a look at them.  First, we see that `imdb` is a `dictionary`-class object and `info` is a `tensorflow_datasets.core.dataset_info.DatasetInfo`-class object 

```{python}
type(imdb)
type(info)
```

We can access a list of attribues available for each of these objects using `dir()` and then view these attribues by converting each list to a Pandas `DataFrame`. These attributes let us know what sub-objects are stored within `imdb_dir` and `info_dir`, respectively, and what embedded methods exist.

```{python}
imdb_dir = dir(imdb)
info_dir = dir(info)
```

```{python}
import pandas as pd

pd.DataFrame(imdb_dir, columns = ['Attributes'])
```

```{python}
pd.DataFrame(info_dir, columns = ['Attributes'])
```

We see that `imdb` contains the `keys` attribute.  This will allows us to see what keyed objects names exist within the data object. 

```{python}
imdb.keys()
```

Each of the keyed objects are iterables containing the sentences used for training or testing along with the labels where each are stored as tensors. Previously, we used the tokenizer and padding methods extracted from Keras. If we want to keep using these methods (we do) we need to convert the data into a format such that these methods will work.  

From the output of the chunk above we see that the available keys are `test`, `train`, and `unsupervised`. Note that the data are split already split where 25,000 reviews are used for training and 25,000 reviews are used for testing. We then extract the values assigned to each key and store them as an objects named `train_data`, `test_data` and  `unsp`, repectively. 

```{python}
train_data, test_data, unsp = imdb['train'], imdb['test'], imdb['unsupervised']

train_data.output_shapes

test_data.output_shapes

unsp.output_shapes
```

## Preparing the data

Now, we define empty lists that will be used to contain the sentences and labels for both training and testing data. 

```{python}
import numpy as np

training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []
```

Then, we iterate over training data extracting
the sentences and the labels. The values for `s` and `l` are tensors, so calling their embedded NumPy method will extract their value, this is shown in the code chunk below.

```{python}
for s,l in train_data:
  training_sentences.append(str(s.numpy()))
  training_labels.append(l.numpy())

for s,l in train_data:
  testing_sentences.append(str(s.numpy()))
  testing_labels.append(l.numpy())
```

When we get to the point that were ready to run our neural network we'll need to ensure that the labels are stored as `numpy` arrays. So, let's go ahead convert our lists labels of into `numpy` arrays using `array` method in the `numpy` module.

```{python}
training_labels_final = np.array(training_labels)

testing_labels_final = np.array(testing_labels)
```

In the following chunk, we tokenize our sentences, convert the tokens to sequences of key : value pairs and pad the sequences to be the same length using code that's similar to what we saw before.  The only difference here is that the parameters are defined at the top as it's easier to change and edit them. for the literals and then changing those. 

```{python}
vocab_size = 10000
max_length = 120
trunc_type = "post"
oov_tok = "<OOV>"

# import the tokenizer and pad_sequences methods
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# create an instance of tokenizer, giving it our  
# vocab_size and desired out of vocab token
tokenizer = Tokenizer(num_words = vocab_size, 
                      oov_token = oov_tok)

# fit the tokenizer on training_sentences
tokenizer.fit_on_texts(training_sentences)

# define our word_index
word_index = tokenizer.word_index

# with word_index defined we can replace the strings 
# containing the words with the token value created for them
sequences = tokenizer.texts_to_sequences(training_sentences)

# sentences have variable length, so we'll pad and/or truncate 
# the sequenced sentences until they're all the same length
# using the maxlength parameter.
padded = pad_sequences(sequences,
                       maxlen = max_length, 
                       truncating = trunc_type)
```

Do the same for the testing sequences. Note that the `word_index` is words that were derived from the training set, so you should expect to see a lot more `<OOV>` tokens in `testing_sequences`.

```{python}
testing_sequences = tokenizer.texts_to_sequences(testing_sentences)

testing_padded = pad_sequences(testing_sequences, 
                               maxlen = max_length)
```

## Building the Neural Network

Now it's time to define our neural network, as shown in the code chunk below.  Take a look at the code at we'll unpack it line-by-line in the next paragraph.

```{python}
embedding_dim = 16

model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size,
                            embedding_dim,
                            input_length = max_length),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(6,activation = "relu"),
  tf.keras.layers.Dense(1,activation = "sigmoid")
])

model.summary()
```

### Line 1: <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential">`tf.keras.Sequential([])`</a>

This line is fairly self-explanatory.  We extract the `Sequential()` method from the `keras` module that is part of the `tensorflow` library.  The brackets `[]` indicate that is takes a list of values as arguments.  In reality, these values are functions that are implement sequentially.  As result, this `Sequential()` applies the enclosed functions sequentially, such that the output of one function is used as input is used an an input to the next function.

### Line 2: <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding">`tf.keras.layers.Embedding`</a>

This embedding function is the key to performing text sentiment analysis in TensorFlow.  You can think of what this embedding function is doing like this:

- Words in a sentence that have similar meanings are often located close to each other. So a movie review might say that the movie was dull and boring, or it might say that it was fun and exciting. What if we could pick a vector in a higher-dimensional space such that words found together are given similar vectors. 
- By combining many reviews we see that words can begin to cluster together. The meaning of the words can come from the labeling of the dataset. So in this case, we say a negative review and the words dull and boring show up a lot in the negative review so that they have similar sentiments, and they are close to each other in the sentence. Thus their vectors will be similar. 
- As the neural network trains, it can learn these vectors associating them with the labels to come up with what's called an embedding i.e., the vectors for each word with their associated sentiment. 

The results of the embedding will be a 2D array with the length of the sentence and the embedding dimension for example 16 as its size.

### Line 3: <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten">`tf.keras.layers.Flatten`</a>

This function is used to convert the 2D embedding array into a format that can be used in the neural network. So we need to flatten it out so that we can feed it into a dense neural network to do the classification. Often in natural language processing, a different layer type, <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D">`tf.keras.layers.GlobalAveragePooling1D`</a> is used in this step.  This function averages across the vector to flatten it out. The reason for using this type of flattening method is because the resulting object is simpler and should allow the model the fit a little faster.

```{python}
model2 = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, 
                            embedding_dim, 
                            input_length = max_length),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(units = 6,activation = "relu"),
  tf.keras.layers.Dense(units = 1,activation = "sigmoid")
])

model2.summary()
```

### Line 4: <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">`tf.keras.layers.Dense`</a>

This layer is just your regular densely-connected NN layer.

## Compiling and Fitting the Neural Network

Next, we must compile our neural network, defining the type of optimizer we can to use, the loss function, and the any output metrics.  For this model we use the binary cross-entropy loss function that is commonly used for any binary classification task, you can check out <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/losses">**this site**</a> to learn more about the classes of loss functions available in `tensorflow`.  We also choose to use the `adam` optimizer, that implements the Adam algorithm.  You can learn more about the `adam` optimizer used within `tensorflow` <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam">**here**</a> and can see what other optimizers are available by visiting <a target=" " herf="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">**this site**</a>.  Finally, we request that our model return metrics on the accuracy of our model, visit <a target=" " href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics">**this site**</a> to learn more about the types of metrics that are available to be returned from a `tensorflow` model.

```{python}
model.compile(loss = 'binary_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])
```

With our model compiled we can fit the neural network to our training data by passing `padded` and `training_label final`, specifying the number of epochs, and then passing `testing_padded` and `testing_labels` as our validation data, or test data set. The model results are shown below. We see that with we achieve an accuracy of 1.00 on the training data and accuracy of 0.8798 on the validation data -- it would appear that we've likely overfit the training data.

```{python}
num_epochs = 10

history = model.fit(padded,
                    training_labels_final,
                    epochs = num_epochs,
                    validation_data = (testing_padded, testing_labels_final))
```

Plot stuff

```{python}
import matplotlib.pyplot as plt

def plot_graphs(history,string):
  plt.plot(history.history[string])
  plt.plot(history.history["val_"+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, "val_"+string])
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")
```


We'll look into how to address overfitting later, for now let's look into visualizing 
the embeddings using the <a target=" " href="http://projector.tensorflow.org/">**TensorFlow embeddings projector**</a>. To do this we'll need to write the embedding vectors and their corresponding metadata to   files that can be uploaded to the TensorFlow Projector to plot the vectors in 3D space so we can visualize them.

First, we need to get the results of the embeddings layer (i.e. the weights and the shape).  Since the embeddings layer is layer zero in our model we can get the results using the code below.  

```{python}
e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)
```

From the result of this chunk we can see that this is a 10,000 by 16 array.  This is because we have 10,000 words in our corpus, and we previously defined `embedding_dim = 16`, so our embedding will have that shape.  

To plot our embeddings, we'll need a helper function to reverse our word index. As it currently stands, our word index has the key being the word, and the value being the token for the word. We'll need to flip this around, to look through the padded list to decode the tokens back into the words.  The helper function below can be used to decode the words from their numerical representations.

```{python}
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(decode_review(padded[1]))
print(training_sentences[1])
```

The code below uses the io module to open two files called `vecs.tsv` and `meta.tsv` and the writes the vector embeddings and metadata to these files.

```{python}
import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()
```

To now render the embeddings follow the steps below:

1. Go to the TensorFlow Embedding Projector on projector.tensorflow.org
2. Press the 'Load data' button on the left. 
3. You'll see a dialog asking you to load data from your computer. 
4. Use vector.TSV for the first one, and meta.TSV for the second. 
5. Once they're loaded, you should see something like this. 
6. Click this 'sphereize data' checkbox on the top left, and you'll see the binary
clustering of the data. 

Experiment by searching for words, or clicking on the blue dots in the chart that represent words. You can search for words to see which ones match a classification. So for example, if I search for boring, we can see that it lights up in one of the clusters and that associated words were clearly negative such as unwatchable. Similarly, if I search for a negative word like annoying, I'll find it along
with annoyingly in the cluster that's clearly the negative reviews. Or if I search for fun, I'll find that fun and funny are positive, fundamental is neutral, and unfunny is of course, negative. 